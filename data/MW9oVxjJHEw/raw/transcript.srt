1
00:00:00,000 --> 00:00:05,780
So, I'm Joel, and I'm going to talk about what is the simplest possible thing that might

2
00:00:05,780 --> 00:00:08,260
work and why didn't you try that first.

3
00:00:08,260 --> 00:00:14,140
So first I tried to ask chat GPT to write my talk, but it gave me one of its mealy-mouthed,

4
00:00:14,140 --> 00:00:16,520
you know, it's complicated answers.

5
00:00:16,520 --> 00:00:20,300
If you're playing norm conf bingo, I hope you have chat GPT on your card a lot, because

6
00:00:20,300 --> 00:00:24,060
that's probably the only one that you're going to get, but you'll get it a lot of times.

7
00:00:24,060 --> 00:00:25,980
So anyway, like I said, I'm Joel.

8
00:00:25,980 --> 00:00:28,340
I wrote a book called Data Science from Scratch, which was mentioned.

9
00:00:28,340 --> 00:00:31,980
I also more recently wrote a book called 10 Essays on Fizz Buzz, which is interesting.

10
00:00:31,980 --> 00:00:34,060
You should check it out.

11
00:00:34,060 --> 00:00:38,700
I'm known sometimes for my live coding videos, and more infamously, I'm the guy who does

12
00:00:38,700 --> 00:00:40,780
not like Jupyter notebooks.

13
00:00:40,780 --> 00:00:44,220
And so if you don't like Jupyter notebooks, you can watch my talk and agree.

14
00:00:44,220 --> 00:00:47,180
Or if you do like them, you can watch my talk and not agree.

15
00:00:47,180 --> 00:00:50,100
And I have a podcast, Abracadabra Learning, which I didn't plug because we never make

16
00:00:50,100 --> 00:00:53,340
new episodes anymore, but maybe we will.

17
00:00:53,340 --> 00:00:57,900
So most of you have never worked with me, but if you had, you would know that my favorite

18
00:00:57,900 --> 00:01:01,900
people to ask questions or my favorite question to ask people about the problems is, what

19
00:01:01,900 --> 00:01:04,900
is the simplest thing that might possibly work?

20
00:01:04,900 --> 00:01:09,340
And the reason I started asking that is because people like to make things overly complicated.

21
00:01:09,340 --> 00:01:12,820
For example, not too long ago, I was talking with someone who wanted to do a machine learning

22
00:01:12,820 --> 00:01:15,280
project involving book reviews.

23
00:01:15,280 --> 00:01:18,060
And so they needed book reviews.

24
00:01:18,060 --> 00:01:21,180
And so they started asking questions about like, what's the best way to build a Goodreads

25
00:01:21,180 --> 00:01:22,660
scraper?

26
00:01:22,660 --> 00:01:29,140
And if you look, building a Goodreads scraper is a non-trivial endeavor.

27
00:01:29,140 --> 00:01:32,020
And so I looked in the mirror and asked myself, what is the simplest thing that might possibly

28
00:01:32,020 --> 00:01:33,020
work?

29
00:01:33,020 --> 00:01:36,220
And that was, hey, maybe someone else has already scraped this dataset and shared it.

30
00:01:36,220 --> 00:01:39,380
And sure enough, it turns out that they have.

31
00:01:39,380 --> 00:01:43,460
And so that was potentially a better solution, a simpler solution.

32
00:01:43,460 --> 00:01:44,620
And chatGPT knows this.

33
00:01:44,620 --> 00:01:47,940
If you asked it how to get the book reviews data, it will suggest downloading it.

34
00:01:47,940 --> 00:01:49,700
I'm very bullish on chatGPT.

35
00:01:49,700 --> 00:01:52,100
I would be even more bullish if it weren't broken.

36
00:01:52,100 --> 00:01:56,460
Half the time I asked it to try to slide, but big things are coming.

37
00:01:56,460 --> 00:02:00,060
And so over the course of my career, I've spent a lot of time teaching diverse teams

38
00:02:00,060 --> 00:02:03,620
of junior data scientists to do good data science.

39
00:02:03,620 --> 00:02:07,020
I've done this so much that if you ask Dally to draw a picture of someone doing this, it

40
00:02:07,020 --> 00:02:09,940
draws a picture that looks almost exactly like me.

41
00:02:09,940 --> 00:02:12,540
And so my style is to go off and let people try things.

42
00:02:12,540 --> 00:02:15,180
And then they try things and they come back and they tell me about them.

43
00:02:15,180 --> 00:02:17,820
And so then I have to ask, what's the simplest thing that might possibly work?

44
00:02:17,820 --> 00:02:19,660
And why didn't you try that first?

45
00:02:19,660 --> 00:02:22,300
And so I kept asking that a lot.

46
00:02:22,300 --> 00:02:26,220
But the further I go, I find that simplicity is not...

47
00:02:26,220 --> 00:02:31,460
Well, one, it's not easy to get an AI to draw it, but also it's not easy to define.

48
00:02:31,460 --> 00:02:35,700
And so, you know, here's this quote about simplicity requires hard work to achieve and

49
00:02:35,700 --> 00:02:37,100
complexity sells better.

50
00:02:37,100 --> 00:02:40,660
So maybe simplicity is sort of the opposite of complexity.

51
00:02:40,660 --> 00:02:43,260
Okay, well, then what's complexity?

52
00:02:43,260 --> 00:02:47,660
Well, you know, if you do machine learning, you've probably seen this bias-variance trade-off

53
00:02:47,660 --> 00:02:52,100
graph where as your model gets more complex, the bias goes down and the variance goes up.

54
00:02:52,100 --> 00:02:57,100
And here, complexity means something like, you know, number of parameters or model capacity.

55
00:02:57,100 --> 00:02:58,260
Is that a good way to think about it?

56
00:02:58,260 --> 00:03:01,780
That's probably not how I want to think about it, but we'll see.

57
00:03:01,780 --> 00:03:06,140
And you can find examples of, you know, here are some models that are simpler in the model

58
00:03:06,140 --> 00:03:09,060
capacity sense that do better than models that are more complex.

59
00:03:09,060 --> 00:03:11,700
And, you know, people will write about these.

60
00:03:11,700 --> 00:03:14,500
That's not where I kind of want to go.

61
00:03:14,500 --> 00:03:20,860
Rather, I would like to really seek what is simplicity and then you can distrust what

62
00:03:20,860 --> 00:03:22,860
I say.

63
00:03:22,860 --> 00:03:27,060
So a big part of why I ended up thinking about this problem is that for the last five plus

64
00:03:27,060 --> 00:03:29,780
years I've been doing a lot of natural language processing.

65
00:03:29,780 --> 00:03:33,700
And so I'm always talking to people about their natural language processing problems.

66
00:03:33,700 --> 00:03:36,300
And so I talked to a lot of people about text classification.

67
00:03:36,300 --> 00:03:38,700
It's come up a couple of times today already.

68
00:03:38,700 --> 00:03:42,540
And so if you're not an LP person, text classification is exactly what it sounds like.

69
00:03:42,540 --> 00:03:45,420
Trying to decide whether email is spam or not.

70
00:03:45,420 --> 00:03:48,700
Trying to decide whether a piece of text has positive sentiment or negative sentiment or

71
00:03:48,700 --> 00:03:49,700
neither.

72
00:03:49,700 --> 00:03:53,060
Trying to decide whether a blog comment is toxic or whether a talk is worth attending

73
00:03:53,060 --> 00:03:56,020
based on its title or its abstract or its presenter.

74
00:03:56,020 --> 00:03:58,940
Or whether a machine learning model is simple based on its description.

75
00:03:58,940 --> 00:04:00,740
So there's a lot of ways you could proceed.

76
00:04:00,740 --> 00:04:04,660
You know, one easy approach might be logistic regression.

77
00:04:04,660 --> 00:04:08,740
Take each email, convert it to a feature vector somehow using like bag of words or one-hot

78
00:04:08,740 --> 00:04:13,740
decoding or averaging word vectors and then you learn some weights, one per feature.

79
00:04:13,740 --> 00:04:17,180
So that's basically like one parameter for each word in your vocabulary.

80
00:04:17,180 --> 00:04:21,380
So you know, that's reasonably simple.

81
00:04:21,380 --> 00:04:24,500
Another very classic approach is what's called naive Bayes classification.

82
00:04:24,500 --> 00:04:27,660
I don't want to go through the math, but you chop your email into words.

83
00:04:27,660 --> 00:04:31,380
You do some computation about how likely you are to see a given word in the spam email

84
00:04:31,380 --> 00:04:32,380
or not.

85
00:04:32,380 --> 00:04:37,340
And then you apply this PIPA-BEBE theorem and then you know, you get it out.

86
00:04:37,340 --> 00:04:39,580
This is kind of the classical approach to spam detection.

87
00:04:39,580 --> 00:04:41,340
There's a chapter on my book.

88
00:04:41,340 --> 00:04:44,100
You could implement it yourself in an afternoon.

89
00:04:44,100 --> 00:04:51,580
And it again has like N parameters where that's the number of words.

90
00:04:51,580 --> 00:04:54,980
About five years ago, the state of the art would have been, you know, this less dim,

91
00:04:54,980 --> 00:04:59,540
tat-dim model where you take your email, convert it to a sequence of embeddings, run them through

92
00:04:59,540 --> 00:05:02,660
this recurrent neural network, get a final state, classify that hidden state.

93
00:05:02,660 --> 00:05:06,100
You know, this is now suddenly thousands and thousands of parameters for all these matrices

94
00:05:06,100 --> 00:05:09,620
that you're multiplying by.

95
00:05:09,620 --> 00:05:12,420
And you know, more recently you do a BERT model, which I'm using in shorthand for any

96
00:05:12,420 --> 00:05:15,700
kind of transformer model, where you take the email, convert it to a bunch of different

97
00:05:15,700 --> 00:05:21,140
embeddings, feed it to the transformer, get some pre-trained embedding as input to the

98
00:05:21,140 --> 00:05:26,300
classifier, fine tune it, and now you're talking like hundreds of millions of parameters.

99
00:05:26,300 --> 00:05:31,740
And so, you know, if I could see you and listen to you, I would ask you a question and I would

100
00:05:31,740 --> 00:05:37,140
say, which of these is the simplest approach to test classification?

101
00:05:37,140 --> 00:05:40,740
Logistic regression, Naive Bayes, LSTM, or BERT?

102
00:05:40,740 --> 00:05:44,540
And so there's no audience feedback, but I'm just going to assume that you're all voting

103
00:05:44,540 --> 00:05:48,060
for choice number one.

104
00:05:48,060 --> 00:05:52,420
So I don't know, I don't think there's a right answer.

105
00:05:52,420 --> 00:05:53,420
People can differ.

106
00:05:53,420 --> 00:05:54,620
But now here's a different question.

107
00:05:54,620 --> 00:05:58,460
Let's say you had a friend who was a junior data scientist and they needed to do some

108
00:05:58,460 --> 00:06:01,900
kind of test classification and they needed to get good results and their job depended

109
00:06:01,900 --> 00:06:04,580
on them getting good results.

110
00:06:04,580 --> 00:06:06,460
What would you recommend?

111
00:06:06,460 --> 00:06:08,100
And there I know what the answer is.

112
00:06:08,100 --> 00:06:10,460
I would recommend that they use BERT.

113
00:06:10,460 --> 00:06:13,520
And I know that's the answer because I keep meeting people who are doing test classification

114
00:06:13,520 --> 00:06:19,980
using Naive Bayes or, you know, logistic regression or XGBoost on TFIDF vectors or other things

115
00:06:19,980 --> 00:06:21,660
you don't even want to know about.

116
00:06:21,660 --> 00:06:24,540
And I'm like, why did you not just use BERT?

117
00:06:24,540 --> 00:06:29,020
And so it's not just that I tell them to use BERT, it's like, why are you wasting your

118
00:06:29,020 --> 00:06:30,780
time doing the others?

119
00:06:30,780 --> 00:06:32,380
But then there's this tension, right?

120
00:06:32,380 --> 00:06:37,100
Like on one hand, you have this angel on my shoulder saying, try the simplest thing first.

121
00:06:37,100 --> 00:06:41,900
And then you also have this devil that's like, why did you just not use BERT?

122
00:06:41,900 --> 00:06:47,980
And so I felt like there was this contradiction there, right?

123
00:06:47,980 --> 00:06:50,580
And so what do we do when we feel like we have a contradiction?

124
00:06:50,580 --> 00:06:54,660
You know, you turn to Anne Rand and she says, contradictions do not exist.

125
00:06:54,660 --> 00:06:58,940
When you think you have one, check your premises and one of them is wrong.

126
00:06:58,940 --> 00:07:03,140
So which of my premises is wrong?

127
00:07:03,140 --> 00:07:06,700
Well, let's take a step back.

128
00:07:06,700 --> 00:07:11,020
When we did that LSTM model, I said that it had thousands of parameters from these matrices

129
00:07:11,020 --> 00:07:13,580
that were multiplying things by and so on and so forth.

130
00:07:13,580 --> 00:07:18,380
But the reason that that model only has thousands of parameters is that it's kind of free riding

131
00:07:18,380 --> 00:07:22,540
off of whatever text embedding model you're using, GloVe vectors or Word2Vec.

132
00:07:22,540 --> 00:07:26,660
If you go and set out to download the GloVe vectors themselves, that's going to be hundreds

133
00:07:26,660 --> 00:07:28,620
of megabytes of data.

134
00:07:28,620 --> 00:07:33,580
So maybe those should count for part of the complexity of the model that, you know, in

135
00:07:33,580 --> 00:07:38,720
this model, you have millions of parameters worth of data that choose which word goes

136
00:07:38,720 --> 00:07:39,980
to which vector.

137
00:07:39,980 --> 00:07:44,020
But most of those parameters are already fixed by the time you show up.

138
00:07:44,020 --> 00:07:47,580
Someone had to learn those vectors once, but then you can use them over and over again

139
00:07:47,580 --> 00:07:52,180
for different problems, just changing some parameters here and there.

140
00:07:52,180 --> 00:07:55,160
So they're a black box that's not really part of our model.

141
00:07:55,160 --> 00:07:57,060
Do they count as complexity?

142
00:07:57,060 --> 00:07:58,060
They're pretty complex.

143
00:07:58,060 --> 00:08:03,540
Or do they count as simplicity because they move the complexity kind of out of our modeling

144
00:08:03,540 --> 00:08:07,980
and into the pre-processing, if you will.

145
00:08:07,980 --> 00:08:16,620
And so, you know, if we go back to BERT, you could also think of the BERT embeddings as

146
00:08:16,620 --> 00:08:18,500
a black box.

147
00:08:18,500 --> 00:08:24,020
So I think a BERT is something where I give it, you know, a sentence and it spits out

148
00:08:24,020 --> 00:08:25,020
an embedding.

149
00:08:25,020 --> 00:08:26,020
Okay.

150
00:08:26,020 --> 00:08:30,020
And now I take that embedding, use it as input to some simple classifier, logistic regression

151
00:08:30,020 --> 00:08:31,420
or whatever.

152
00:08:31,420 --> 00:08:33,020
And now I fine tune this.

153
00:08:33,020 --> 00:08:38,020
And that part of the model itself, that's thousands of parameters, not millions of parameters,

154
00:08:38,020 --> 00:08:39,020
right?

155
00:08:39,020 --> 00:08:42,900
And then like, you know, I'm cheating a little bit because when you fine tune, you are like

156
00:08:42,900 --> 00:08:47,580
updating those BERT weights somewhat, so they're not quite a black box or like a gray box.

157
00:08:47,580 --> 00:08:51,780
But there's actually, outside of BERT computing those embeddings for you, there's not a lot

158
00:08:51,780 --> 00:08:52,780
going on.

159
00:08:52,780 --> 00:08:58,940
In fact, there's less going on there than in the, you know, fancy LSTM model.

160
00:08:58,940 --> 00:09:02,500
And what's more, there's also some hidden complexity in the simple models like Naive

161
00:09:02,500 --> 00:09:03,500
Bayes.

162
00:09:03,500 --> 00:09:07,100
So, you know, imagine you're doing this Naive Bayes where I want to chop my email into words.

163
00:09:07,100 --> 00:09:09,140
Well, maybe I also want to consider phrases.

164
00:09:09,140 --> 00:09:11,900
So now I need to, you know, look at n-grams rather than words.

165
00:09:11,900 --> 00:09:13,580
And maybe I want to filter out stop words.

166
00:09:13,580 --> 00:09:17,260
And maybe I would like to do some kind of stemming so that classifier and classification

167
00:09:17,260 --> 00:09:19,900
show up as basically the same word.

168
00:09:19,900 --> 00:09:21,500
Maybe I want to split out contractions.

169
00:09:21,500 --> 00:09:24,260
Maybe I want to do any number of things.

170
00:09:24,260 --> 00:09:28,580
And so, you know, once you get into the mechanics of I want to do a simple Naive Bayes model

171
00:09:28,580 --> 00:09:35,740
on this, you have to make a lot of choices and you have a lot of degrees of freedom.

172
00:09:35,740 --> 00:09:40,180
And suddenly, if you include those degrees of freedom in your model, it's less simple

173
00:09:40,180 --> 00:09:42,020
with, you know, one of these transformer models.

174
00:09:42,020 --> 00:09:46,500
You just kind of shove the text in and it does with it what it does with it to kind

175
00:09:46,500 --> 00:09:51,660
of go back to what we were talking about earlier about find methods that can cope with your

176
00:09:51,660 --> 00:09:55,940
dirty data and just ignore it.

177
00:09:55,940 --> 00:10:01,660
So this is a lot of clutter.

178
00:10:01,660 --> 00:10:06,500
So where do we find the simplicity?

179
00:10:06,500 --> 00:10:08,260
And you know, I mentioned woodworking at the start.

180
00:10:08,260 --> 00:10:15,300
So now I'm actually going to talk about woodworking and hopefully it will be relevant and make

181
00:10:15,300 --> 00:10:16,460
sense.

182
00:10:16,460 --> 00:10:22,340
So as I mentioned, you know, in the pre-talk banter, woodworking is sort of a new hobby

183
00:10:22,340 --> 00:10:23,340
of mine.

184
00:10:23,340 --> 00:10:27,420
I only started doing it in earnest a month or two ago.

185
00:10:27,420 --> 00:10:31,620
But what I've been doing all year is watching woodworking videos on YouTube.

186
00:10:31,620 --> 00:10:35,980
In fact, I'm not bragging when I say that I'm pretty much like a savant at watching

187
00:10:35,980 --> 00:10:37,540
woodworking videos on YouTube.

188
00:10:37,540 --> 00:10:42,940
Like I'm really, you know, I'm a 10X woodworking video watcher, if you will.

189
00:10:42,940 --> 00:10:44,500
As a woodworker, I'm not that good.

190
00:10:44,500 --> 00:10:48,040
I barely know how to use the tools, but good at the watching videos.

191
00:10:48,040 --> 00:10:54,140
So now imagine that you and your friends are like building a dining table, right?

192
00:10:54,140 --> 00:10:59,420
And so like these people, I think they're people, you have no idea what you're doing,

193
00:10:59,420 --> 00:11:04,340
but you know, you don't want it to have these sort of ugly, simple square legs.

194
00:11:04,340 --> 00:11:08,300
You want your table to be beautiful and have these complex tapered legs with, you know,

195
00:11:08,300 --> 00:11:12,540
nice slopes that intersect each other and that look good.

196
00:11:12,540 --> 00:11:16,300
And so, you know, one thing you can do, obviously what's the simplest thing you can do, and

197
00:11:16,300 --> 00:11:20,340
why do you not do that first is you can go to Amazon and buy tapered legs.

198
00:11:20,340 --> 00:11:22,540
It'll cost you 175 bucks.

199
00:11:22,540 --> 00:11:29,460
But if you just want legs and you just want to buy them, then you can do it that way.

200
00:11:29,460 --> 00:11:32,580
It works, it's expensive.

201
00:11:32,580 --> 00:11:37,060
But it is an intricate and involved process.

202
00:11:37,060 --> 00:11:39,340
And there's a lot of ways that it can go wrong.

203
00:11:39,340 --> 00:11:42,380
If you're not like a master craftsman like this guy, like this is a video of someone,

204
00:11:42,380 --> 00:11:43,540
you know, making tapered legs.

205
00:11:43,540 --> 00:11:44,860
He's really good at what he does.

206
00:11:44,860 --> 00:11:49,060
And he's got all these hand tools and he makes this precise cuts and, you know, it turns

207
00:11:49,060 --> 00:11:51,460
out beautifully.

208
00:11:51,460 --> 00:11:56,500
But then you watch more YouTube videos and you start learning about jigs.

209
00:11:56,500 --> 00:12:02,520
And so a jig is sort of like a tool that you build to take away some of your complexity.

210
00:12:02,520 --> 00:12:04,620
So this here is a tapering jig.

211
00:12:04,620 --> 00:12:09,300
I want to cut a piece of wood at a specific angle to a specific length.

212
00:12:09,300 --> 00:12:12,020
So I build this little frame that I can put my wood in.

213
00:12:12,020 --> 00:12:15,200
It has a stop so that the wood can only go in really one place.

214
00:12:15,200 --> 00:12:19,220
And it has another stop that holds the wood at exactly one angle.

215
00:12:19,220 --> 00:12:23,260
And you put a piece of wood in, you run it through your table saw, and you get the exact,

216
00:12:23,260 --> 00:12:25,620
you know, slope and size that you want.

217
00:12:25,620 --> 00:12:28,700
And it's repeatable and it always works.

218
00:12:28,700 --> 00:12:32,140
And so all you have to do is, you know, however many of these pieces you want.

219
00:12:32,140 --> 00:12:35,380
You shove a piece of wood in it, you run it through the table saw, and you try not to

220
00:12:35,380 --> 00:12:38,340
cut your fingers off and, you know, you're done.

221
00:12:38,340 --> 00:12:45,060
And it's suggested to me that how simple something is, is not just how complex it is in theory,

222
00:12:45,060 --> 00:12:49,660
but also how difficult is it for you to try it?

223
00:12:49,660 --> 00:12:55,260
So you know, if it's 2018 and you wanted to fine tune a BERT model, you were signing up

224
00:12:55,260 --> 00:12:56,260
for a lot of work.

225
00:12:56,260 --> 00:12:57,260
This is one of the examples.

226
00:12:57,260 --> 00:13:03,540
This is a text classification example from PyTorch pre-trained BERT, which was like the

227
00:13:03,540 --> 00:13:06,940
original precursor that became Hugging Face eventually.

228
00:13:06,940 --> 00:13:10,340
And if you wanted to make it work on your data, you were signing up for a minor adventure.

229
00:13:10,340 --> 00:13:16,220
This is like line 532 of the text classification example.

230
00:13:16,220 --> 00:13:20,700
And so, you know, many, many hundreds of lines of code.

231
00:13:20,700 --> 00:13:24,180
In 2022, if you want to fine tune BERT, like this is basically it.

232
00:13:24,180 --> 00:13:30,020
You import a couple of things from Transformers and, you know, create a tokenizer, create

233
00:13:30,020 --> 00:13:34,220
a padding collator, create a model, train it, and train.

234
00:13:34,220 --> 00:13:39,380
And like, I've left out a few lines, but this is like basically it, right?

235
00:13:39,380 --> 00:13:42,300
These days, you know, in 2018, it was not simple to use BERT.

236
00:13:42,300 --> 00:13:44,140
It was not simple to fine tune a BERT model.

237
00:13:44,140 --> 00:13:46,860
Today it is pretty simple to use a BERT model.

238
00:13:46,860 --> 00:13:51,140
And in fact, I would argue it's simpler to get good results by fine tuning a BERT model

239
00:13:51,140 --> 00:13:55,100
than it is to get good results with a simpler model.

240
00:13:55,100 --> 00:13:58,140
And if you just want like the pre-trained embeddings out of it, it's even simpler.

241
00:13:58,140 --> 00:13:59,500
It's basically three lines of code.

242
00:13:59,500 --> 00:14:02,980
You import from Sentence Transformers, load a model, and say encode my sentences, and

243
00:14:02,980 --> 00:14:03,980
you're done.

244
00:14:03,980 --> 00:14:07,020
So, there's like nothing to it.

245
00:14:07,020 --> 00:14:10,500
So maybe when I talk about simplicity, I mean not just a model that's itself simple, but

246
00:14:10,500 --> 00:14:13,420
a model by which it's simple to get good results.

247
00:14:13,420 --> 00:14:19,260
So, you know, one other example that's slightly out of the ML data science space is sorting.

248
00:14:19,260 --> 00:14:20,780
Sorting algorithms can be really simple.

249
00:14:20,780 --> 00:14:24,780
You know, sometimes if you interview canonically, they'll ask you to implement, you know, bubble

250
00:14:24,780 --> 00:14:27,620
sort or merge sort or quick sort or whatever.

251
00:14:27,620 --> 00:14:31,300
You know, back when I was preparing for coding interviews back in the day, I memorized like

252
00:14:31,300 --> 00:14:33,140
every one known to man.

253
00:14:33,140 --> 00:14:35,980
Not that that's really done me any good.

254
00:14:35,980 --> 00:14:39,540
And Python uses an algorithm called TimSort.

255
00:14:39,540 --> 00:14:43,180
You know, my guess is that most of you don't know how TimSort works, because I don't know

256
00:14:43,180 --> 00:14:44,500
how TimSort works.

257
00:14:44,500 --> 00:14:45,780
It's slightly complicated.

258
00:14:45,780 --> 00:14:46,780
It's not obvious.

259
00:14:46,780 --> 00:14:50,020
It's not a simple algorithm.

260
00:14:50,020 --> 00:14:52,660
And yet it's simple to use, right?

261
00:14:52,660 --> 00:14:56,180
So if you say, if you ask a chat GPT, what's the simplest sorting algorithm?

262
00:14:56,180 --> 00:14:57,900
It says probably bubble sort.

263
00:14:57,900 --> 00:14:58,900
And I think that's right.

264
00:14:58,900 --> 00:15:03,460
Probably conceptually, you know, if I had to explain it to my 11 year old, I would pick

265
00:15:03,460 --> 00:15:04,460
bubble sort.

266
00:15:04,460 --> 00:15:09,220
But if you're working in Python, you should of course use the built in sorted algorithm

267
00:15:09,220 --> 00:15:14,900
that you used to, unless you have like a super good reason not to, right?

268
00:15:14,900 --> 00:15:17,220
And then if you ask it, you know, is TimSort a simple algorithm?

269
00:15:17,220 --> 00:15:21,620
It says, no, it's not a simple algorithm.

270
00:15:21,620 --> 00:15:23,420
But is it simple to use correctly?

271
00:15:23,420 --> 00:15:28,160
Yes, it's very simple to use correctly, right?

272
00:15:28,160 --> 00:15:31,740
Unless you have a comparison function like the other day in the advent of code, which

273
00:15:31,740 --> 00:15:33,380
it was less simple to use correctly.

274
00:15:33,380 --> 00:15:38,060
But anyway, the point I'm trying to make is that as our tools get better, the boundary

275
00:15:38,060 --> 00:15:40,380
between simple and complex changes.

276
00:15:40,380 --> 00:15:45,280
So how easy a solution is to implement and scale depends a great deal on the tools and

277
00:15:45,280 --> 00:15:47,420
abstractions you have for working with it.

278
00:15:47,420 --> 00:15:52,300
With a tapering jig, it's simple to implement and scale making tapered legs.

279
00:15:52,300 --> 00:15:55,520
If you don't have a jig like that, it's not so simple.

280
00:15:55,520 --> 00:15:59,380
With the Python standard library, it's simple to use a stable hybrid merge insertion sort,

281
00:15:59,380 --> 00:16:01,300
which is what TimSort is.

282
00:16:01,300 --> 00:16:04,460
You know, in other languages, it's not so simple.

283
00:16:04,460 --> 00:16:07,940
So I saw an interesting tweet about this from the CEO of AngelList Venture.

284
00:16:07,940 --> 00:16:10,020
He says, why use AngelList to run a fund?

285
00:16:10,020 --> 00:16:13,700
He says, my answer, AngelList abstracts away all the complexity.

286
00:16:13,700 --> 00:16:16,860
So he said it was starting and running a fund.

287
00:16:16,860 --> 00:16:18,780
And so I found that phrase really interesting, right?

288
00:16:18,780 --> 00:16:21,300
It's not that it eliminates the complexity.

289
00:16:21,300 --> 00:16:23,000
Complexity can't be gotten rid of.

290
00:16:23,000 --> 00:16:28,220
But it abstracts it all away and gives you an API where for the most part, you don't

291
00:16:28,220 --> 00:16:29,540
have to worry about it.

292
00:16:29,540 --> 00:16:32,200
And from your perspective, it's much simpler.

293
00:16:32,200 --> 00:16:37,180
So sometimes things can be simple, not because they lack complexity, but because complexity

294
00:16:37,180 --> 00:16:39,860
has been abstracted away.

295
00:16:39,860 --> 00:16:44,980
And so, you know, I guess the lesson here is that as you're doing data science or machine

296
00:16:44,980 --> 00:16:50,020
learning or software engineering or data engineering, whatever, think about ways to abstract away

297
00:16:50,020 --> 00:16:51,700
the complexity.

298
00:16:51,700 --> 00:16:55,060
Sometimes that can be, you know, just using pre-trained models and user-friendly libraries.

299
00:16:55,060 --> 00:17:00,300
Sometimes you might want to create your own shared product, project templates, you know,

300
00:17:00,300 --> 00:17:07,660
crafting clean APIs, using engineering best practices and using shared processes.

301
00:17:07,660 --> 00:17:15,740
And then if you do that, you can use complex machinery in a norm-conf way.

302
00:17:15,740 --> 00:17:20,380
So there was a survey that was floating around the other day about ML and AI practices where

303
00:17:20,380 --> 00:17:23,940
like a third of the people said they were using natural language understanding, but

304
00:17:23,940 --> 00:17:26,780
only a third of those people said they were using transformers.

305
00:17:26,780 --> 00:17:30,940
And now, you know, maybe they didn't know because they say, oh, I use Hugging Face.

306
00:17:30,940 --> 00:17:32,620
I don't know.

307
00:17:32,620 --> 00:17:37,860
But when I see that, I think, wow, that's, you know, two-thirds of the people using natural

308
00:17:37,860 --> 00:17:42,580
language understanding are probably not getting as good results as they could with the same

309
00:17:42,580 --> 00:17:43,580
amount of work.

310
00:17:43,580 --> 00:17:49,700
So, you know, you have my blessing to go import transformers and go nuts, and I won't criticize

311
00:17:49,700 --> 00:17:51,180
you.

312
00:17:51,180 --> 00:17:54,220
And even Chad GPDT knows this one, right?

313
00:17:54,220 --> 00:17:57,580
If you ask it, what's the simplest way to solve a text classification problem, you know,

314
00:17:57,580 --> 00:17:58,580
it's just a bag of words.

315
00:17:58,580 --> 00:18:02,580
But then it says another option is to use a pre-trained language model to generate embeddings

316
00:18:02,580 --> 00:18:06,620
for each document and then train a classifier for these embeddings.

317
00:18:06,620 --> 00:18:08,860
So take it from Chad GPDT.

318
00:18:08,860 --> 00:18:11,900
That's a simple way to solve the problem.

319
00:18:11,900 --> 00:18:15,020
This is one of the woodworking YouTubers that I follow.

320
00:18:15,020 --> 00:18:17,860
He put out a video pretty recently about making a simple, elegant table.

321
00:18:17,860 --> 00:18:21,740
And of course, I saw the word simple and like it keyed with me.

322
00:18:21,740 --> 00:18:26,540
And you know, he said simplicity is something that sometimes we only discover through experience

323
00:18:26,540 --> 00:18:28,500
and with confidence.

324
00:18:28,500 --> 00:18:29,820
And that resonated with me, right?

325
00:18:29,820 --> 00:18:34,660
Like knowing what the complexity, what the complex parts and the simple parts are is

326
00:18:34,660 --> 00:18:38,900
how you know how to abstract those complex parts away and leave yourself with something

327
00:18:38,900 --> 00:18:42,160
simple to work with.

328
00:18:42,160 --> 00:18:48,700
And so I'll leave you with a quote by Lao Tzu about abstracting complexity away.

329
00:18:48,700 --> 00:18:52,500
It's not really about that, but there aren't too many quotes about abstracting complexity

330
00:18:52,500 --> 00:18:53,500
away.

331
00:18:53,500 --> 00:18:58,780
Manifest plainness, embrace simplicity, reduce selfishness, have few desires.

332
00:18:58,780 --> 00:19:03,380
So manifest plainness, that means don't put too many stickers on your laptop.

333
00:19:03,380 --> 00:19:06,340
There's too many, take them off.

334
00:19:06,340 --> 00:19:09,020
Reduce selfishness, that means don't hog all the GPUs, share them.

335
00:19:09,020 --> 00:19:10,940
Other people need to use them too.

336
00:19:10,940 --> 00:19:15,100
Everyone needs to train machine learning models.

337
00:19:15,100 --> 00:19:17,060
Have few desires, I don't really understand this one.

338
00:19:17,060 --> 00:19:18,060
I don't know.

339
00:19:18,060 --> 00:19:19,060
Someone can explain it to me later.

340
00:19:19,060 --> 00:19:20,900
I don't get it.

341
00:19:20,900 --> 00:19:23,400
And then embrace simplicity, right?

342
00:19:23,400 --> 00:19:28,460
So if there's a moral to this talk, it's maybe that simplicity isn't simple.

343
00:19:28,460 --> 00:19:31,800
Things that seem simple sometimes aren't, and things that seem complex might actually

344
00:19:31,800 --> 00:19:34,440
be a simpler choice.

345
00:19:34,440 --> 00:19:35,440
So thank you.

346
00:19:35,440 --> 00:19:36,440
I'm Joel.

347
00:19:36,440 --> 00:19:37,440
You can find me on Twitter.

348
00:19:37,440 --> 00:19:38,440
Everyone keeps saying that Twitter is dying.

349
00:19:38,440 --> 00:19:39,440
It's not dying.

350
00:19:39,440 --> 00:19:40,440
It's still there.

351
00:19:40,440 --> 00:19:41,680
It's like the same as always, pretty much.

352
00:19:41,680 --> 00:19:44,220
So I'm still there.

353
00:19:44,220 --> 00:19:55,960
And that's that.

