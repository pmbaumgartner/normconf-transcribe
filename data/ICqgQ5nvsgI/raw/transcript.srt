1
00:00:00,000 --> 00:00:02,440
Thank you all for having me here today.

2
00:00:02,440 --> 00:00:05,920
I'm super excited to be talking about the physics of data,

3
00:00:05,920 --> 00:00:08,880
two of my favorite subjects combined.

4
00:00:08,880 --> 00:00:11,960
So let's talk about how I actually got here.

5
00:00:11,960 --> 00:00:15,760
I got into computers through physics and math.

6
00:00:16,600 --> 00:00:18,680
The world of physics and math is so wonderful

7
00:00:18,680 --> 00:00:22,000
because the world is modeled through equations

8
00:00:22,000 --> 00:00:24,620
that reflect what's going on in the world beautifully.

9
00:00:24,620 --> 00:00:29,440
You don't actually need to get into a lot of

10
00:00:29,440 --> 00:00:32,900
the intricacies of thinking about how a computer functions

11
00:00:32,900 --> 00:00:34,400
when you're doing those computations.

12
00:00:34,400 --> 00:00:36,200
Generally, when I was working in the world

13
00:00:36,200 --> 00:00:39,080
of physics and math, I thought about working in industry.

14
00:00:39,080 --> 00:00:42,080
And in industry, I really had this image of like,

15
00:00:42,080 --> 00:00:44,600
oh, you don't really need to understand how,

16
00:00:44,600 --> 00:00:47,600
like you're just working at algorithms and making models

17
00:00:47,600 --> 00:00:49,400
and letting computers compute.

18
00:00:49,400 --> 00:00:51,920
But you then join industry and it's like

19
00:00:51,920 --> 00:00:54,760
horrifying reality hits you one day.

20
00:00:54,760 --> 00:00:56,040
You know, someone comes up to you and they're like,

21
00:00:56,040 --> 00:00:57,620
why is your job so slow?

22
00:00:57,620 --> 00:00:59,560
Why does it cost so much?

23
00:00:59,560 --> 00:01:02,480
Or maybe you're trying to escape from the dinosaurs

24
00:01:02,480 --> 00:01:05,080
that are free in the theme park you're in.

25
00:01:05,080 --> 00:01:07,000
And you realize you really need to know

26
00:01:07,000 --> 00:01:09,560
how a computer functions to actually do your job.

27
00:01:10,760 --> 00:01:13,920
So let's start first with relative orders of magnitude

28
00:01:13,920 --> 00:01:16,080
because these orders of magnitude of operations

29
00:01:16,080 --> 00:01:18,440
really drive the constraints of the system.

30
00:01:18,440 --> 00:01:21,600
So in 2012, Jeff Dean published a list of numbers

31
00:01:21,600 --> 00:01:24,140
called Numbers Everyone Should Know.

32
00:01:24,140 --> 00:01:24,980
I have it here on the left.

33
00:01:24,980 --> 00:01:27,480
It's kind of small, so I pulled out.

34
00:01:27,480 --> 00:01:29,720
Some of the operations that I think are interesting.

35
00:01:29,720 --> 00:01:31,040
There's a few things to notice here.

36
00:01:31,040 --> 00:01:34,440
Number one, it takes a while to send data somewhere

37
00:01:34,440 --> 00:01:35,800
over a network and get it back.

38
00:01:35,800 --> 00:01:37,600
And it takes longer the longer the distance.

39
00:01:37,600 --> 00:01:41,000
So a data sent around trip versus a packet from California

40
00:01:41,000 --> 00:01:42,760
to another one to California again.

41
00:01:44,020 --> 00:01:47,560
Reading data, also slow, but a sequential read

42
00:01:47,560 --> 00:01:49,960
of approximately a megabyte is only twice

43
00:01:50,960 --> 00:01:53,200
the length of time as a disk speed

44
00:01:53,200 --> 00:01:56,160
to actually like go find data on a disk somewhere.

45
00:01:56,160 --> 00:01:58,800
And then I kind of summarize these like CPU operations,

46
00:01:58,800 --> 00:02:01,120
things that are close to CPU and RAM,

47
00:02:01,120 --> 00:02:03,200
where you have a L1 cache reference

48
00:02:03,200 --> 00:02:05,520
or a branch misprediction in CPU.

49
00:02:05,520 --> 00:02:07,280
These are fast.

50
00:02:07,280 --> 00:02:08,800
So when we think about data,

51
00:02:08,800 --> 00:02:10,600
the first thing we want to recognize

52
00:02:10,600 --> 00:02:12,640
is that we want to move it around as little as possible

53
00:02:12,640 --> 00:02:14,120
because it's incredibly slow.

54
00:02:14,120 --> 00:02:15,520
I've included a link down here

55
00:02:15,520 --> 00:02:17,040
and you can get access to my slides later

56
00:02:17,040 --> 00:02:18,520
for a great reference where you can see

57
00:02:18,520 --> 00:02:19,840
these numbers over time.

58
00:02:19,840 --> 00:02:22,480
These numbers are old, they're 2012.

59
00:02:22,480 --> 00:02:24,560
That's really the relative magnitude

60
00:02:24,560 --> 00:02:26,680
that I'm interested in.

61
00:02:26,680 --> 00:02:30,440
So let's start with some very norm data processing.

62
00:02:31,440 --> 00:02:32,720
If you work at a company,

63
00:02:32,720 --> 00:02:35,400
very norm data processing means you first query data

64
00:02:35,400 --> 00:02:38,140
that lives in the data warehouse somewhere.

65
00:02:38,140 --> 00:02:41,200
Usually this is data spread across a few servers,

66
00:02:41,200 --> 00:02:42,680
maybe there's a bunch of different tables,

67
00:02:42,680 --> 00:02:45,080
maybe you have a large amount of data.

68
00:02:45,080 --> 00:02:46,520
And then after some querying,

69
00:02:46,520 --> 00:02:48,160
you maybe aggregate it to a point

70
00:02:48,160 --> 00:02:50,200
that you're gonna do some math on a single machine.

71
00:02:50,200 --> 00:02:51,280
Maybe it's your laptop,

72
00:02:51,280 --> 00:02:53,800
maybe it's some math in the data warehouse

73
00:02:53,800 --> 00:02:56,240
or in the cloud, whatever that is.

74
00:02:56,240 --> 00:02:57,760
So I'm gonna break this up into two parts.

75
00:02:57,760 --> 00:02:59,680
The first part, I'm gonna talk about querying data

76
00:02:59,680 --> 00:03:01,980
in the data warehouse and implications there.

77
00:03:03,280 --> 00:03:06,200
So the question is how fast can we move?

78
00:03:06,200 --> 00:03:08,240
And this is kind of about the physics of data,

79
00:03:08,240 --> 00:03:10,520
but also about pictures that I think are funny.

80
00:03:10,520 --> 00:03:13,520
And so IBM has this photo

81
00:03:13,520 --> 00:03:16,540
that they are letting me use very generously.

82
00:03:16,540 --> 00:03:18,520
I planned it up in advance to ask.

83
00:03:18,520 --> 00:03:23,520
This is four dudes moving a raw Mac 305 into a truck

84
00:03:24,760 --> 00:03:28,760
because this is how they move five megabytes at a time.

85
00:03:28,760 --> 00:03:31,460
Very difficult, it can only go as fast as a text scan.

86
00:03:31,460 --> 00:03:32,480
Well, actually that's not quite true.

87
00:03:32,480 --> 00:03:34,360
You could also load it into a plane

88
00:03:34,360 --> 00:03:35,660
and a plane would be faster.

89
00:03:35,660 --> 00:03:37,720
And I think maybe an interesting thing to note here

90
00:03:37,720 --> 00:03:42,720
is that the cylindrical object there

91
00:03:42,840 --> 00:03:45,640
is actually a disc of magnetic platters

92
00:03:45,640 --> 00:03:50,480
that record the bits, the bytes of data

93
00:03:50,480 --> 00:03:52,700
of this data processing machine.

94
00:03:52,700 --> 00:03:55,040
And that's how they're read off of them.

95
00:03:55,040 --> 00:03:56,520
And so this might remind you of something.

96
00:03:56,520 --> 00:03:59,200
This is like a closeup of a raw Mac 305

97
00:03:59,200 --> 00:04:02,520
at the Computer History Museum that they're restoring.

98
00:04:02,520 --> 00:04:07,240
And these like cylindrical shaped platter disc

99
00:04:07,240 --> 00:04:10,240
is where the image of the database logo icon

100
00:04:10,240 --> 00:04:12,000
that you see in every architecture diagram

101
00:04:12,000 --> 00:04:15,240
actually gets its image from.

102
00:04:15,240 --> 00:04:17,240
So much as the kids of today don't know

103
00:04:17,240 --> 00:04:19,640
that the save icon is a floppy disc,

104
00:04:19,640 --> 00:04:21,880
I didn't really know this was related

105
00:04:21,880 --> 00:04:22,880
until someone told me.

106
00:04:22,880 --> 00:04:24,620
So I hope you enjoy that.

107
00:04:25,840 --> 00:04:26,680
And so you might think to yourself,

108
00:04:26,680 --> 00:04:28,640
oh, surely technology has improved.

109
00:04:28,640 --> 00:04:31,000
Things have gotten so much faster.

110
00:04:31,000 --> 00:04:33,800
But in fact, if you have about a hundred petabytes

111
00:04:33,800 --> 00:04:36,200
and you need to move some data into AWS,

112
00:04:36,200 --> 00:04:38,320
they will send you a semi truck

113
00:04:38,320 --> 00:04:39,840
and take all of your data,

114
00:04:39,840 --> 00:04:42,720
load it into the semi truck and drive it

115
00:04:42,720 --> 00:04:44,360
to their data center.

116
00:04:44,360 --> 00:04:47,280
So while it is much faster to move five megabytes,

117
00:04:47,280 --> 00:04:48,960
there's still bandwidth limits.

118
00:04:55,440 --> 00:04:57,400
So what does this actually mean

119
00:04:57,400 --> 00:04:59,000
for querying the data warehouse?

120
00:05:00,000 --> 00:05:00,840
Well, first of all,

121
00:05:00,840 --> 00:05:02,600
we want to move things as little as possible.

122
00:05:02,600 --> 00:05:05,200
And so that means doing work as close as possible

123
00:05:05,200 --> 00:05:06,400
to where the data lives.

124
00:05:09,080 --> 00:05:10,800
Netflix actually does exactly this.

125
00:05:10,800 --> 00:05:13,560
And I'm not gonna talk to our data processing team at all.

126
00:05:13,560 --> 00:05:15,400
Instead of Netflix,

127
00:05:15,400 --> 00:05:18,200
you watch movies at home on your TVs,

128
00:05:18,200 --> 00:05:21,240
you stream those movies to your TVs from somewhere.

129
00:05:21,240 --> 00:05:22,080
And like I just said,

130
00:05:22,080 --> 00:05:24,040
it's actually quite expensive and difficult

131
00:05:24,040 --> 00:05:26,120
to move large amounts of data around.

132
00:05:26,120 --> 00:05:29,240
And so when Netflix has done and built its own CDN,

133
00:05:29,240 --> 00:05:31,320
we know that we have people all over the world

134
00:05:31,320 --> 00:05:33,000
that will be reading these files,

135
00:05:33,000 --> 00:05:34,560
streaming the data to them.

136
00:05:34,560 --> 00:05:36,400
So a very read heavy workload.

137
00:05:36,400 --> 00:05:39,040
We feel very comfortable making copies of these files

138
00:05:39,040 --> 00:05:39,880
all over the world.

139
00:05:39,880 --> 00:05:41,520
So we actually partner with ISPs,

140
00:05:41,520 --> 00:05:44,320
put little boxes inside of the ISPs

141
00:05:44,320 --> 00:05:46,960
and cache films that we think people nearby

142
00:05:46,960 --> 00:05:48,160
will want to see.

143
00:05:48,160 --> 00:05:49,560
And when you request it,

144
00:05:49,560 --> 00:05:51,160
you actually stream it from somewhere much closer.

145
00:05:51,160 --> 00:05:52,720
And so the distance that we're sending data

146
00:05:52,720 --> 00:05:54,320
is much shorter.

147
00:05:54,320 --> 00:05:55,720
And so I think this is kind of fascinating

148
00:05:55,720 --> 00:05:58,200
because it is a read heavy workload,

149
00:05:58,200 --> 00:06:00,000
much like analytic workloads.

150
00:06:00,000 --> 00:06:01,520
It doesn't really matter if we have copies

151
00:06:01,520 --> 00:06:03,640
and you're not like trying to make changes or transactions.

152
00:06:03,640 --> 00:06:06,400
This is kind of exactly the concept of data locality,

153
00:06:06,400 --> 00:06:08,320
except distributed all over the world.

154
00:06:08,320 --> 00:06:11,040
So let's talk a little bit more about data locality.

155
00:06:16,200 --> 00:06:18,000
Data locality means a few things.

156
00:06:18,000 --> 00:06:20,160
One is that you've organized your data really well

157
00:06:20,160 --> 00:06:22,880
so that your work and your processing happens close by.

158
00:06:22,880 --> 00:06:24,800
So good data organization,

159
00:06:24,800 --> 00:06:26,800
but also the best kind of data

160
00:06:26,800 --> 00:06:28,640
is less data than you have today.

161
00:06:29,640 --> 00:06:33,280
And so you really, you know, data, wonderful, great,

162
00:06:33,280 --> 00:06:34,720
but maybe think about storing lots of it,

163
00:06:34,720 --> 00:06:36,400
both for data governance, data privacy,

164
00:06:36,400 --> 00:06:40,560
but also reading one megabyte sequentially off disk.

165
00:06:40,560 --> 00:06:42,120
It doesn't matter how you've imported it.

166
00:06:42,120 --> 00:06:43,880
It doesn't matter how you've compressed it.

167
00:06:43,880 --> 00:06:45,560
You still have all these CPU cycles

168
00:06:45,560 --> 00:06:47,200
relative to reading off of this.

169
00:06:47,200 --> 00:06:51,000
And so having good encoding and compression

170
00:06:51,000 --> 00:06:53,320
can really help us storing literally less of it.

171
00:06:54,960 --> 00:06:56,920
Let's talk about what a hard drive actually looks like.

172
00:06:56,920 --> 00:07:00,680
In the raw Mac 305, we saw these magnetic platters.

173
00:07:01,560 --> 00:07:04,560
In computers, at least, I mean,

174
00:07:04,560 --> 00:07:07,120
nowadays on a laptop, you'll get an SSD.

175
00:07:07,120 --> 00:07:08,720
Whole other story, but also like Asterisk,

176
00:07:08,720 --> 00:07:11,360
kind of not sequential reads are still faster on SSD.

177
00:07:13,040 --> 00:07:16,400
On a hard drive, we have here this magnetic disk

178
00:07:16,400 --> 00:07:19,400
and we have a reader head, this yellow,

179
00:07:19,400 --> 00:07:22,400
like that yellow area that will move to the position

180
00:07:22,400 --> 00:07:23,800
that it needs to read.

181
00:07:23,800 --> 00:07:25,640
The disk is spinning and it has to wait

182
00:07:25,640 --> 00:07:28,040
until the right position comes around again

183
00:07:28,040 --> 00:07:29,880
to begin the sequential read.

184
00:07:29,880 --> 00:07:32,000
So really we're trying to move around the reader head

185
00:07:32,000 --> 00:07:33,280
as little as possible.

186
00:07:33,280 --> 00:07:35,280
And just read sequentially.

187
00:07:35,280 --> 00:07:36,880
So that brings up the question like,

188
00:07:36,880 --> 00:07:39,080
well, if we want to read sequentially,

189
00:07:39,080 --> 00:07:41,480
what are the workloads that we're usually doing?

190
00:07:42,320 --> 00:07:44,280
Let's talk about table layouts.

191
00:07:44,280 --> 00:07:46,600
We have a few tables, there's rows and columns

192
00:07:46,600 --> 00:07:48,400
in these tables.

193
00:07:48,400 --> 00:07:50,520
We might want to aggregate all of these,

194
00:07:50,520 --> 00:07:52,560
like what is the average of column B?

195
00:07:52,560 --> 00:07:57,560
Maybe that's how much a certain transaction costs.

196
00:07:58,200 --> 00:08:01,680
Maybe column C is like what state or country people are in.

197
00:08:01,680 --> 00:08:03,480
Maybe we want to group by.

198
00:08:03,480 --> 00:08:06,160
And so we're doing scans over these columns, right?

199
00:08:09,880 --> 00:08:11,320
There's a few different ways we can think about

200
00:08:11,320 --> 00:08:13,480
actually storing these data in a file.

201
00:08:13,480 --> 00:08:15,640
One is we could take every row

202
00:08:15,640 --> 00:08:17,320
and store the rows in sequence,

203
00:08:17,320 --> 00:08:18,680
or we could take every column

204
00:08:18,680 --> 00:08:20,280
and store the columns in sequence.

205
00:08:20,280 --> 00:08:22,120
Here we can see that if we were to store the columns

206
00:08:22,120 --> 00:08:24,280
in sequence, if we wanted to take the average of B,

207
00:08:24,280 --> 00:08:25,560
we could do a sequential read

208
00:08:25,560 --> 00:08:28,080
if all of these were immediately after one another.

209
00:08:28,080 --> 00:08:30,880
So this actually now gets us to the place of,

210
00:08:30,880 --> 00:08:32,680
hey, have you heard of Parquet?

211
00:08:32,680 --> 00:08:33,720
Why does Parquet exist?

212
00:08:33,720 --> 00:08:35,160
Parquet exists.

213
00:08:35,160 --> 00:08:37,160
This is often a default file format

214
00:08:37,160 --> 00:08:38,640
that you'll see all over the place

215
00:08:38,640 --> 00:08:40,440
in larger data warehouses.

216
00:08:40,440 --> 00:08:44,200
But the principles behind it apply in many situations.

217
00:08:44,200 --> 00:08:46,120
First, it allows a lot of different compression,

218
00:08:46,120 --> 00:08:50,040
well, three very useful lossless compression algorithms.

219
00:08:50,040 --> 00:08:52,120
It has a few different types of encoding,

220
00:08:52,120 --> 00:08:53,680
and both the encoding and compression

221
00:08:53,680 --> 00:08:55,680
can be applied to a single data set.

222
00:08:55,680 --> 00:08:58,560
And both the encoding and compression can be applied

223
00:08:58,560 --> 00:09:00,560
at a column level.

224
00:09:00,560 --> 00:09:03,320
And so that is incredibly useful for us

225
00:09:03,320 --> 00:09:06,800
because again, we're doing column scans usually.

226
00:09:06,800 --> 00:09:08,840
And it's column oriented

227
00:09:08,840 --> 00:09:10,320
because it's organized in a column way.

228
00:09:10,320 --> 00:09:13,040
Like again, an asterisk kind of,

229
00:09:13,040 --> 00:09:15,080
it's actually, it has the concept of row groups,

230
00:09:15,080 --> 00:09:16,680
but that's neither here nor there.

231
00:09:16,680 --> 00:09:18,360
I got 20 minutes, time to move on.

232
00:09:19,800 --> 00:09:22,200
Second section, we're gonna do math in a single machine.

233
00:09:22,200 --> 00:09:23,560
Great, we have some kind of concept

234
00:09:23,560 --> 00:09:26,120
about how we sort of spread our data around

235
00:09:26,120 --> 00:09:28,000
in a data warehouse and process on it.

236
00:09:28,000 --> 00:09:30,600
Now we're gonna do something more with mathematics.

237
00:09:31,560 --> 00:09:33,120
Single machine computation.

238
00:09:35,400 --> 00:09:36,240
Probably the,

239
00:09:38,360 --> 00:09:40,360
apologies, I'm a little, getting a cold.

240
00:09:42,000 --> 00:09:43,320
Probably the thing that comes up the most

241
00:09:43,320 --> 00:09:45,720
when thinking about computation on a single machine,

242
00:09:45,720 --> 00:09:48,440
you can imagine we've already read the data off of Zip.

243
00:09:50,120 --> 00:09:53,040
Moore's law just comes up all the time, right?

244
00:09:53,040 --> 00:09:56,000
The density of transistors just keeps increasing,

245
00:09:56,000 --> 00:09:59,040
but I hate to break it to you.

246
00:09:59,040 --> 00:10:01,360
Moore's law is topping out.

247
00:10:01,360 --> 00:10:03,280
We're kind of getting to the end of it.

248
00:10:03,280 --> 00:10:06,320
So what are we gonna do about exactly?

249
00:10:06,320 --> 00:10:11,320
Well, first we're going to use wider SIMDs.

250
00:10:11,760 --> 00:10:15,760
SIMDs are a very special type of processing unit

251
00:10:15,760 --> 00:10:18,920
inside of a CPU, where normally in a CPU,

252
00:10:18,920 --> 00:10:21,880
you have registers that can use instructions

253
00:10:21,880 --> 00:10:24,120
and data input and then write the data somewhere.

254
00:10:24,120 --> 00:10:25,840
So let's say you want to take a number,

255
00:10:25,840 --> 00:10:28,200
put it in a register, you have an instruction,

256
00:10:28,200 --> 00:10:30,880
add three to this, and then you're gonna write this

257
00:10:30,880 --> 00:10:32,040
out somewhere else.

258
00:10:32,040 --> 00:10:34,480
So that's like three CPU clock cycles.

259
00:10:34,480 --> 00:10:38,200
SIMDs let you do that with groups of registers.

260
00:10:38,200 --> 00:10:41,200
So it's like four registers

261
00:10:41,200 --> 00:10:43,600
and four of exactly the same instructions.

262
00:10:43,600 --> 00:10:48,600
So that's same instruction, multiple data.

263
00:10:48,640 --> 00:10:51,200
So you apply the same instruction to like these vectors.

264
00:10:51,200 --> 00:10:54,120
So again, like sounding a whole lot like

265
00:10:54,120 --> 00:10:58,280
the type of sequential, let's do a bunch of things

266
00:10:58,280 --> 00:10:59,720
parallelized at once.

267
00:10:59,720 --> 00:11:01,840
So the wider the SIMD, the more registered,

268
00:11:01,840 --> 00:11:06,320
but it has the bigger vectorized calculations you can do.

269
00:11:06,320 --> 00:11:08,840
It also have multiple core processors.

270
00:11:08,840 --> 00:11:10,120
I think people talk a lot more

271
00:11:10,120 --> 00:11:11,520
about multiple core processing.

272
00:11:11,520 --> 00:11:12,880
So I'm just gonna skip it in this talk

273
00:11:12,880 --> 00:11:14,920
and go a little bit deeper into SIMDs.

274
00:11:16,120 --> 00:11:18,840
Or you can have higher clock frequency.

275
00:11:18,840 --> 00:11:21,800
And this is where it gets kind of fascinating

276
00:11:21,800 --> 00:11:23,120
and challenging.

277
00:11:23,120 --> 00:11:27,160
When you look at what is constraining CPUs

278
00:11:27,160 --> 00:11:28,000
in a number of ways.

279
00:11:28,000 --> 00:11:29,960
One is purely the size of the transistors.

280
00:11:29,960 --> 00:11:31,320
And the second is the amount of power

281
00:11:31,320 --> 00:11:33,440
you need to actually do computation.

282
00:11:34,680 --> 00:11:38,440
You don't wanna spend a lot on energy.

283
00:11:38,440 --> 00:11:40,040
I mean, you already do, but you don't wanna spend more

284
00:11:40,040 --> 00:11:41,080
than you have to.

285
00:11:41,080 --> 00:11:43,760
But also when you have power going across these transistors,

286
00:11:43,760 --> 00:11:46,920
it's letting off heat and you might melt it,

287
00:11:46,920 --> 00:11:48,280
like literally.

288
00:11:48,280 --> 00:11:52,360
And so data centers use a lot of HVAC.

289
00:11:52,360 --> 00:11:55,160
And so their costs are both powering all of their computers,

290
00:11:55,160 --> 00:11:57,040
but also cooling it so nothing melts.

291
00:11:58,680 --> 00:12:03,360
Wider SIMDs scales linearly with power.

292
00:12:03,360 --> 00:12:05,640
Multi-core processors scales quadratically

293
00:12:05,640 --> 00:12:08,360
and a higher clock frequency scales cubically.

294
00:12:08,360 --> 00:12:10,840
So while all processors have started moving

295
00:12:10,840 --> 00:12:12,640
and increasing these directions,

296
00:12:12,640 --> 00:12:15,320
wider SIMDs are where the power trade-off

297
00:12:15,320 --> 00:12:16,360
makes the most sense.

298
00:12:16,360 --> 00:12:17,360
Cool.

299
00:12:17,360 --> 00:12:18,800
Is that the only thing we can do?

300
00:12:18,800 --> 00:12:19,640
Absolutely not.

301
00:12:19,640 --> 00:12:21,720
There's lots of really cool processors

302
00:12:21,720 --> 00:12:24,880
that people have made to make things a little bit better,

303
00:12:24,880 --> 00:12:28,320
faster, and specifically less power.

304
00:12:28,320 --> 00:12:29,640
It's both financial reasons,

305
00:12:29,640 --> 00:12:31,120
but if you want another incentive,

306
00:12:31,120 --> 00:12:33,440
like do you like polar bears in the Antarctic,

307
00:12:33,440 --> 00:12:34,720
you should use less energy.

308
00:12:35,960 --> 00:12:37,240
One, which I've just talked about,

309
00:12:37,240 --> 00:12:38,960
is wider SIMDs and CPUs.

310
00:12:40,240 --> 00:12:41,920
Second is GPUs.

311
00:12:41,920 --> 00:12:44,680
So often these are used for video game processors

312
00:12:44,680 --> 00:12:46,920
or fancy deep learning things.

313
00:12:46,920 --> 00:12:47,760
They're kind of interesting

314
00:12:47,760 --> 00:12:50,360
because they're still very generic

315
00:12:50,360 --> 00:12:51,960
in the sense that they can use

316
00:12:51,960 --> 00:12:53,520
multiple types of instructions.

317
00:12:53,520 --> 00:12:55,400
So they're somewhat flexible.

318
00:12:55,400 --> 00:12:56,400
And then ASICs,

319
00:12:56,400 --> 00:12:59,400
which are application-specific integrated circuits.

320
00:12:59,400 --> 00:13:01,200
And the fanciest example of this,

321
00:13:01,200 --> 00:13:03,640
and probably the one that I think is the fanciest

322
00:13:03,640 --> 00:13:04,880
because it's related to deep learning,

323
00:13:04,880 --> 00:13:07,000
are Google's TPUs.

324
00:13:07,000 --> 00:13:08,840
And these are tensor processing units,

325
00:13:08,840 --> 00:13:12,240
which literally all they do is linear algebra all day.

326
00:13:12,240 --> 00:13:13,840
It's like matrix multiplication.

327
00:13:13,840 --> 00:13:16,600
It's like matrix multiplication after matrix multiplication.

328
00:13:16,600 --> 00:13:18,800
And there's a really great power trade-off.

329
00:13:20,360 --> 00:13:21,400
That's all I can talk about it

330
00:13:21,400 --> 00:13:25,480
because deep learning is too not-norm-close.

331
00:13:25,480 --> 00:13:26,640
I'm done.

332
00:13:26,640 --> 00:13:27,480
Great.

333
00:13:27,480 --> 00:13:30,240
So this is a picture of what a data center looks like.

334
00:13:30,240 --> 00:13:32,920
This is a Google data center in the Dalles, Oregon.

335
00:13:32,920 --> 00:13:36,200
And this is specifically where the power grid meets it.

336
00:13:36,200 --> 00:13:37,680
Power is a real concern.

337
00:13:37,680 --> 00:13:39,040
These things cost a lot.

338
00:13:40,440 --> 00:13:43,320
So we're going the wider SIMD track.

339
00:13:43,320 --> 00:13:44,760
How are we going to do this?

340
00:13:44,760 --> 00:13:46,560
There's really kind of three ways

341
00:13:46,560 --> 00:13:48,560
that I think are relatively common.

342
00:13:48,560 --> 00:13:50,040
D, using vector primitives.

343
00:13:50,040 --> 00:13:51,120
Like in the language,

344
00:13:51,120 --> 00:13:51,960
there are vector primitives

345
00:13:51,960 --> 00:13:55,640
that allow you to access SIMD vectorization.

346
00:13:56,560 --> 00:13:59,960
I don't think it comes up that much, but it's possible.

347
00:13:59,960 --> 00:14:01,960
I personally don't write a lot of raw C.

348
00:14:03,280 --> 00:14:06,040
Another example that all of us probably have used

349
00:14:06,040 --> 00:14:09,960
behind the scenes without knowing is Lopak, Slask,

350
00:14:09,960 --> 00:14:12,640
all of these libraries that are linear algebra libraries

351
00:14:12,640 --> 00:14:17,640
written forever ago that are hyper-optimized

352
00:14:17,760 --> 00:14:20,480
and are able to use the vectorization capabilities

353
00:14:20,480 --> 00:14:25,480
inside of the platform that it's been like compiled against.

354
00:14:26,920 --> 00:14:29,080
And so this is actually why a lot of Python

355
00:14:29,080 --> 00:14:30,600
to be a little tricky

356
00:14:30,600 --> 00:14:33,320
because it's not just Python versions that we're managing.

357
00:14:33,320 --> 00:14:35,520
We're also managing these like C and Fortran libraries

358
00:14:35,520 --> 00:14:37,480
that are underlying.

359
00:14:37,480 --> 00:14:39,360
And then the third is LLVM.

360
00:14:39,360 --> 00:14:41,200
LLVM is a tool chain in C

361
00:14:41,200 --> 00:14:43,600
that allows you to take compiled C code

362
00:14:43,600 --> 00:14:46,240
and an intermediate representation of compiled C code

363
00:14:46,240 --> 00:14:49,960
and then optimize the assembly language output.

364
00:14:49,960 --> 00:14:52,840
So let's talk a little bit about LLVM for a second.

365
00:14:52,840 --> 00:14:55,960
Impala is a distributed query engine.

366
00:14:55,960 --> 00:14:57,880
It's written in D.

367
00:14:57,880 --> 00:15:02,200
Skye Wonderman-Nolne, she's an incredible engineer

368
00:15:02,200 --> 00:15:05,040
and she works on vectorization of queries

369
00:15:05,040 --> 00:15:07,880
that were submitted, that gets submitted to Impala,

370
00:15:07,880 --> 00:15:09,200
have huge performance gains

371
00:15:09,200 --> 00:15:12,200
and there's a link to a talk that she gave about it.

372
00:15:12,200 --> 00:15:14,000
You can use LLVM back and flat.

373
00:15:14,000 --> 00:15:15,400
This is super common.

374
00:15:15,400 --> 00:15:17,640
So if you yourself are writing a Python library

375
00:15:17,640 --> 00:15:19,240
that's scientific computing oriented

376
00:15:19,240 --> 00:15:21,640
and you're like, no, I'm not gonna use SciPy

377
00:15:21,640 --> 00:15:23,160
or NumPy for some reason,

378
00:15:24,440 --> 00:15:27,000
you could have your own binding

379
00:15:27,000 --> 00:15:28,440
through using these libraries.

380
00:15:28,440 --> 00:15:30,080
But much more frequently,

381
00:15:30,080 --> 00:15:32,080
you're gonna try to install something

382
00:15:32,080 --> 00:15:34,240
and if you install it correctly, it will just happen.

383
00:15:34,240 --> 00:15:37,240
So these are instructions for a library called Axe.

384
00:15:37,240 --> 00:15:41,240
It's a Bayesian optimization library built on PyTorch

385
00:15:41,240 --> 00:15:43,560
and in it, it's like, hey, be careful about the order

386
00:15:43,560 --> 00:15:44,560
that you're installing things

387
00:15:44,560 --> 00:15:46,280
because if you do it correctly,

388
00:15:46,280 --> 00:15:48,600
it'll be significantly faster and this is why.

389
00:15:48,600 --> 00:15:50,840
So I think that's cool.

390
00:15:51,760 --> 00:15:54,800
In 2007, Gordon Moore was invited

391
00:15:54,800 --> 00:15:58,600
to the Computer History Museum for an interview

392
00:15:58,600 --> 00:16:01,000
and he had this quote, which I think is incredible.

393
00:16:01,000 --> 00:16:02,960
The fact that materials are made of atoms

394
00:16:02,960 --> 00:16:05,760
is the fundamental limitation and it's not that far away.

395
00:16:05,760 --> 00:16:08,400
We're pushing up again some of the fairly fundamental limits.

396
00:16:08,400 --> 00:16:09,240
So one of these days,

397
00:16:09,240 --> 00:16:11,800
we're gonna have to stop making things smaller.

398
00:16:11,800 --> 00:16:14,800
And I hate to break it to you, ladies and gentlemen,

399
00:16:14,800 --> 00:16:17,120
that day is very close.

400
00:16:17,120 --> 00:16:19,440
If you look at transistor size in nanometers,

401
00:16:19,440 --> 00:16:23,280
in 1971, it was about 10 million.

402
00:16:24,440 --> 00:16:28,480
This year, the transistor size is about three nanometers

403
00:16:28,480 --> 00:16:30,640
and the silicon atom is 0.2 nanometers.

404
00:16:30,640 --> 00:16:32,280
So if we look at now transistor size

405
00:16:32,280 --> 00:16:36,000
in terms of silicon atoms, it used to be 50 million

406
00:16:36,000 --> 00:16:40,200
and now we're about 15 atoms across.

407
00:16:40,200 --> 00:16:42,280
And so trying to get electrons to cross there,

408
00:16:42,280 --> 00:16:45,000
it actually begins to make us have to take into account

409
00:16:45,000 --> 00:16:47,080
quantum effects, let alone the fact

410
00:16:47,080 --> 00:16:48,280
the amount of power that it takes

411
00:16:48,280 --> 00:16:50,440
to actually make these transistors function.

412
00:16:50,440 --> 00:16:51,560
It's predicted that next year,

413
00:16:51,560 --> 00:16:54,040
we're gonna get down to two nanometers across,

414
00:16:54,040 --> 00:16:56,200
which is again, incredible,

415
00:16:56,200 --> 00:16:59,120
but we're hitting a real physical reality.

416
00:16:59,120 --> 00:17:00,440
What's kind of exciting is sure,

417
00:17:00,440 --> 00:17:02,280
quantum computers are being built.

418
00:17:02,280 --> 00:17:03,480
This is amazing.

419
00:17:03,480 --> 00:17:05,960
I don't see math immediately happening on them.

420
00:17:05,960 --> 00:17:08,200
It seems like that would be very expensive,

421
00:17:08,200 --> 00:17:12,480
but again, this is a new boundary of computation.

422
00:17:12,480 --> 00:17:15,120
Totally different talk, because again, 20 minutes.

423
00:17:15,120 --> 00:17:19,000
So the question is, data and computation with data

424
00:17:19,000 --> 00:17:20,640
happens on computers.

425
00:17:20,640 --> 00:17:21,960
How do you build your skills?

426
00:17:21,960 --> 00:17:23,240
How do you learn about this?

427
00:17:23,240 --> 00:17:25,520
How do you get better at understanding what is going on

428
00:17:25,520 --> 00:17:27,680
when you run into a problem?

429
00:17:27,680 --> 00:17:32,680
Well, I think that we are, this is like a skilled trade.

430
00:17:34,120 --> 00:17:37,280
And so not only are there things that you can learn,

431
00:17:37,280 --> 00:17:40,640
like computers are not magical, learning is possible.

432
00:17:40,640 --> 00:17:42,840
You need to figure out how to go about doing that.

433
00:17:42,840 --> 00:17:44,440
There's a few books that I think,

434
00:17:44,440 --> 00:17:46,880
well, resources that I think are excellent.

435
00:17:46,880 --> 00:17:49,320
One is Wizard Zines by Bork,

436
00:17:49,320 --> 00:17:51,320
great for like little Unix tools,

437
00:17:51,320 --> 00:17:54,720
extremely fun, delightful to read, strong recommend.

438
00:17:54,720 --> 00:17:55,560
And then the second is

439
00:17:55,560 --> 00:17:58,480
Designing Data-Intensive Applications by Martin Plotman.

440
00:17:58,480 --> 00:18:00,240
I'd be pretty interested to hear in Slack

441
00:18:00,240 --> 00:18:03,920
what other ways you all have best learned

442
00:18:03,920 --> 00:18:05,480
or resources that you've had.

443
00:18:05,480 --> 00:18:08,200
But again, we're skilled craftspeople.

444
00:18:08,200 --> 00:18:09,760
Practice is what makes perfect.

445
00:18:09,760 --> 00:18:11,520
And so if something happens,

446
00:18:11,520 --> 00:18:13,320
like your job runs out of memory,

447
00:18:13,320 --> 00:18:14,880
don't just double your memory and call it good.

448
00:18:14,880 --> 00:18:15,720
What happens?

449
00:18:15,720 --> 00:18:17,520
What does the hypothesis, test it, dig into it,

450
00:18:17,520 --> 00:18:19,600
learn to monitor your system.

451
00:18:19,600 --> 00:18:20,440
But what if you're like,

452
00:18:20,440 --> 00:18:22,360
I don't know how to monitor your system.

453
00:18:22,360 --> 00:18:24,280
Again, like reading, one way to do it.

454
00:18:24,280 --> 00:18:26,840
But my absolute favorite way to learn

455
00:18:26,840 --> 00:18:29,880
about how these systems work is to pair with them.

456
00:18:29,880 --> 00:18:31,840
Someone that's experienced and knows what's going on.

457
00:18:31,840 --> 00:18:33,960
When they're doing some kind of performance debugging,

458
00:18:33,960 --> 00:18:36,480
they're like, oh, this job is slow, what are we gonna do?

459
00:18:36,480 --> 00:18:38,120
And see what tools they use.

460
00:18:38,120 --> 00:18:42,800
I learned just incredible tool sets from people

461
00:18:42,800 --> 00:18:44,680
and like pick up little things here and there

462
00:18:44,680 --> 00:18:46,960
that just begin to add to your speed

463
00:18:46,960 --> 00:18:48,560
and ability to get things done.

464
00:18:48,560 --> 00:18:53,560
Thank you.

