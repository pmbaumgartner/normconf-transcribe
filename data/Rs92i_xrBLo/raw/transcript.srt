1
00:00:00,000 --> 00:00:07,920
I am a data science consultant, especially doing NLP these days, along with a bunch of

2
00:00:07,920 --> 00:00:11,380
other data science-y problems.

3
00:00:11,380 --> 00:00:17,480
I also used to do data visualization and in fact ran a conference called OpenVizConf a

4
00:00:17,480 --> 00:00:21,240
few years ago, which was less well organized than this.

5
00:00:21,240 --> 00:00:23,800
So kudos to you guys.

6
00:00:23,800 --> 00:00:31,860
I have also been a professor at various points, but talk about a difficult job.

7
00:00:31,860 --> 00:00:36,780
And these days I do a lot with creative AI, which my newsletter reflects as well.

8
00:00:36,780 --> 00:00:41,540
So I work part-time with Google Arts and Culture and other clients.

9
00:00:41,540 --> 00:00:44,940
And I'm going to talk a little bit about some of them today, because I've had some interesting

10
00:00:44,940 --> 00:00:47,540
problems with them.

11
00:00:47,540 --> 00:00:51,220
I've got a lot in this deck, and I'm probably going to go kind of fast.

12
00:00:51,220 --> 00:00:54,980
I'm not going to spend a ton of time on the code examples.

13
00:00:54,980 --> 00:01:00,020
But having been a professor, I believe solidly in giving code examples, and I spend a lot

14
00:01:00,020 --> 00:01:02,700
of my days looking for them online.

15
00:01:02,700 --> 00:01:08,420
So there's going to be a bunch of code, including stuff I'm not going to cover in my repo, that

16
00:01:08,420 --> 00:01:10,740
will be linked from these slides.

17
00:01:10,740 --> 00:01:15,780
So you can find how to do things from that.

18
00:01:15,780 --> 00:01:18,740
And this is the essential plan.

19
00:01:18,740 --> 00:01:24,980
The last one, Misk Life Tips, is some of the tools I use that helped me solve some of the

20
00:01:24,980 --> 00:01:27,300
problems I'm going to show today.

21
00:01:27,300 --> 00:01:31,460
And we may not get to it.

22
00:01:31,460 --> 00:01:33,420
We'll see.

23
00:01:33,420 --> 00:01:40,000
So I'm going to use today a bunch of data related to text to image.

24
00:01:40,000 --> 00:01:44,100
So unless you're living under a rock, you know about these models where you can type

25
00:01:44,100 --> 00:01:49,340
in a phrase of what you want to see, and then you get an image out at the end.

26
00:01:49,340 --> 00:01:53,860
Stable diffusion is the most popular open source model.

27
00:01:53,860 --> 00:01:59,700
And there are a bunch of repos of prompts that people have typed in, including this

28
00:01:59,700 --> 00:02:01,700
one, which is Lexica.

29
00:02:01,700 --> 00:02:06,700
So I'm going to use a sample of these for some of my examples today, just so you know

30
00:02:06,700 --> 00:02:10,100
what's coming.

31
00:02:10,100 --> 00:02:14,500
The first technique I want to talk about is called UMAP, which is a data visualization

32
00:02:14,500 --> 00:02:15,500
technique.

33
00:02:15,500 --> 00:02:19,660
This is incredibly useful for exploratory analysis.

34
00:02:19,660 --> 00:02:24,340
I've been using this technique of dimensionality reduction for a while.

35
00:02:24,340 --> 00:02:30,140
And before it was UMAP, it was T-S-N-E, which people call t-S-N-E, which drives me nuts

36
00:02:30,140 --> 00:02:31,140
for some reason.

37
00:02:31,140 --> 00:02:35,660
But anyway, we've got UMAP now, which is a little bit better, although related, as papers

38
00:02:35,660 --> 00:02:37,180
have shown.

39
00:02:37,180 --> 00:02:44,620
And it's something that allows you to get a glimpse of your data, big picture text data,

40
00:02:44,620 --> 00:02:49,000
and really impresses clients, just as a side note.

41
00:02:49,000 --> 00:02:57,780
This is an old, not very pretty or sexy picture that I made using T-S-N-E.

42
00:02:57,780 --> 00:03:04,140
This is a word to Vec embedding of a bunch of poetry on Gutenberg books.

43
00:03:04,140 --> 00:03:05,780
So open source poetry.

44
00:03:05,780 --> 00:03:11,380
Allison Parrish collected all of this and turned it into a corpus that people use for

45
00:03:11,380 --> 00:03:13,580
various creative text projects.

46
00:03:13,580 --> 00:03:19,540
I was using it for a talk a few years ago, and then later was using the same corpus for

47
00:03:19,540 --> 00:03:21,660
a text generation project.

48
00:03:21,660 --> 00:03:28,080
And the first thing I did was this T-S-N-E display of that poetry colored by date.

49
00:03:28,080 --> 00:03:33,980
So this is words, not sentences, embedded using Word2Vec.

50
00:03:33,980 --> 00:03:36,860
And then I mean, it's a blob.

51
00:03:36,860 --> 00:03:40,700
It's not like a good separation of anything.

52
00:03:40,700 --> 00:03:45,640
But the coloring and the sort of shape of it helped me discover something that I had

53
00:03:45,640 --> 00:03:50,660
suspected when I was playing with the data in a generation algorithm.

54
00:03:50,660 --> 00:03:56,740
In particular, this blob on the lower left is like archaic, Latin, old English types

55
00:03:56,740 --> 00:04:01,140
of things, which basically were screwing up a lot of my output in this project.

56
00:04:01,140 --> 00:04:06,220
And having made this picture and done this color coding, I was able to clean the dataset

57
00:04:06,220 --> 00:04:10,180
to turn it into something a little bit more modern, which is what I actually needed.

58
00:04:10,180 --> 00:04:16,420
So this was a tremendously useful display for me at the time.

59
00:04:16,420 --> 00:04:18,140
I do this all the time still.

60
00:04:18,140 --> 00:04:22,300
Now frequently it's with sentences or in this case prompts.

61
00:04:22,300 --> 00:04:29,060
So this is prompts in this online dataset that I'm going to be talking about, where

62
00:04:29,060 --> 00:04:36,420
basically in this case I was looking at a subset with I think the word body was the

63
00:04:36,420 --> 00:04:40,420
subset that I picked for reasons that will become clear.

64
00:04:40,420 --> 00:04:46,580
And I did this interactive display so that I could take a look at how was the word body

65
00:04:46,580 --> 00:04:49,180
appearing in these prompts.

66
00:04:49,180 --> 00:04:52,500
And in particular, is it a bad use?

67
00:04:52,500 --> 00:04:57,100
Because I was interested in not safe for work content detection.

68
00:04:57,100 --> 00:05:02,500
Making these interactive things like the little GIF I just showed is actually incredibly easy

69
00:05:02,500 --> 00:05:03,500
these days.

70
00:05:03,500 --> 00:05:06,140
The tools are just getting better and better.

71
00:05:06,140 --> 00:05:12,180
It's very easy with sentence transformers to just pick whatever model you want.

72
00:05:12,180 --> 00:05:17,540
Body prompts in this code example is just all of the text strings that have the word

73
00:05:17,540 --> 00:05:19,140
body in them.

74
00:05:19,140 --> 00:05:21,180
We do our model encoding.

75
00:05:21,180 --> 00:05:26,540
Then we run it through UMAP, which is this dimensionality reduction algorithm that Leland

76
00:05:26,540 --> 00:05:28,660
McKinnis published.

77
00:05:28,660 --> 00:05:35,100
And that gets us XY coordinates, a two dimensional matrix, which we can turn into an interactive

78
00:05:35,100 --> 00:05:36,780
plot.

79
00:05:36,780 --> 00:05:39,580
Doing the interactive plot code is actually surprisingly easy.

80
00:05:39,580 --> 00:05:41,860
I mean, I'll put this in my repo.

81
00:05:41,860 --> 00:05:45,140
But basically we're going to use this library called Bokeh.

82
00:05:45,140 --> 00:05:49,540
And the thing that's cool about this library in particular is that you can either output

83
00:05:49,540 --> 00:05:53,400
in your notebook, in your Jupyter notebook, or you can make a file.

84
00:05:53,400 --> 00:05:55,060
And the file is self-contained.

85
00:05:55,060 --> 00:05:59,260
So you can send it to a client or show it later or save it in your repo.

86
00:05:59,260 --> 00:06:06,860
And it's fully interactive where you can mouse over the text and see what the text is.

87
00:06:06,860 --> 00:06:09,900
So you can put HTML on the tooltips if you want.

88
00:06:09,900 --> 00:06:15,380
So that's that little hand symbol that you see there.

89
00:06:15,380 --> 00:06:17,940
That's me putting some HTML in.

90
00:06:17,940 --> 00:06:21,100
So it's really, really easy.

91
00:06:21,100 --> 00:06:23,300
This is another couple of examples.

92
00:06:23,300 --> 00:06:28,860
This is a different dataset where I was looking at the use of the word blood in these prompts

93
00:06:28,860 --> 00:06:34,700
because I was concerned about whether blood was being used in a really gory way or whether

94
00:06:34,700 --> 00:06:36,380
it was just sort of generic.

95
00:06:36,380 --> 00:06:40,500
And it turned out, actually, that very few of them were really gory.

96
00:06:40,500 --> 00:06:42,500
A lot of it was very generic.

97
00:06:42,500 --> 00:06:47,940
So in particular, down at the very bottom is a lot of references here to blood moon,

98
00:06:47,940 --> 00:06:49,700
which is just a phrase, right?

99
00:06:49,700 --> 00:06:54,740
It's not especially about blood.

100
00:06:54,740 --> 00:07:00,660
So one of the things I end up doing a lot with these plots is sort of a crappy labeling

101
00:07:00,660 --> 00:07:08,020
where I basically outline what's happening where on the plot in terms of content.

102
00:07:08,020 --> 00:07:10,620
This is painful.

103
00:07:10,620 --> 00:07:17,460
And involves, you know, doing sort of XY coordinates, getting an average, or just doing

104
00:07:17,460 --> 00:07:18,460
it by hand.

105
00:07:18,460 --> 00:07:26,500
So this over here on the right is one where I took this is a bunch of books, and I did

106
00:07:26,500 --> 00:07:30,660
various averaging over the books for genre purposes.

107
00:07:30,660 --> 00:07:33,540
And the authors are the color coding here.

108
00:07:33,540 --> 00:07:38,060
And then I went into Photoshop and I did a really, really careful labeling job with all

109
00:07:38,060 --> 00:07:39,140
the content.

110
00:07:39,140 --> 00:07:40,860
So that takes a long time.

111
00:07:40,860 --> 00:07:44,140
And I mean, hopefully the tools for doing this are going to get better and better.

112
00:07:44,140 --> 00:07:47,380
The libraries for doing this are getting better and better.

113
00:07:47,380 --> 00:07:53,260
It's super powerful when it's presented nicely.

114
00:07:53,260 --> 00:07:55,340
And the interactive side of it is very cool, too.

115
00:07:55,340 --> 00:08:00,820
So you might have seen this thing from Nomic AI, which is actually Ben Schmidt, who's a

116
00:08:00,820 --> 00:08:04,340
digital humanities guy who's essentially a data viz genius.

117
00:08:04,340 --> 00:08:11,060
He did this thing with all of these prompts from CREA.ai's stable diffusion search engine.

118
00:08:11,060 --> 00:08:12,060
It's very cool.

119
00:08:12,060 --> 00:08:14,300
All right.

120
00:08:14,300 --> 00:08:19,980
So relatedly, our previous speaker, Vincent Warmerdon, he's been working on a tool called

121
00:08:19,980 --> 00:08:26,700
bulk that supports selecting from one of these in order to do labeling or to help with labeling

122
00:08:26,700 --> 00:08:30,180
in a kind of weak supervision method, which I had been doing before.

123
00:08:30,180 --> 00:08:35,820
So as soon as he released this library, I sent him like six issues of feature requests.

124
00:08:35,820 --> 00:08:40,540
And then Leland McInnis, the UMAP guy, is also working on a library called this.that,

125
00:08:40,540 --> 00:08:43,500
which might actually end up being better for data viz artifact creation.

126
00:08:43,500 --> 00:08:44,500
I don't know.

127
00:08:44,500 --> 00:08:46,380
I haven't had time to dig into it.

128
00:08:46,380 --> 00:08:49,340
But it looks like it's really cool.

129
00:08:49,340 --> 00:08:55,740
So that's essentially one of my main tips is like if you do these displays, which are

130
00:08:55,740 --> 00:09:01,100
not actually hard to make anymore, you can get a whole lot of mileage out of them.

131
00:09:01,100 --> 00:09:03,720
They're fun and people like them.

132
00:09:03,720 --> 00:09:06,100
And you can make them look really good.

133
00:09:06,100 --> 00:09:07,340
Okay.

134
00:09:07,340 --> 00:09:15,540
So leaving behind the fun and not hard to make, let's talk about like real entity recognition

135
00:09:15,540 --> 00:09:17,140
issues in the wild.

136
00:09:17,140 --> 00:09:24,780
I've had two client issues this past year that are that raised so many issues around

137
00:09:24,780 --> 00:09:31,500
the tooling and the decision making and the design of these problems in this problem space.

138
00:09:31,500 --> 00:09:33,900
So here's one of the client problems.

139
00:09:33,900 --> 00:09:39,100
Suppose you want to see trends in searches in these prompts.

140
00:09:39,100 --> 00:09:45,540
So you have prompts like you see here from Lexica where people are visualizing content,

141
00:09:45,540 --> 00:09:50,340
like in this case, castles, and they're putting in names of artists that they're interested

142
00:09:50,340 --> 00:09:53,460
in using as sort of inspirations.

143
00:09:53,460 --> 00:09:57,600
We all know that this is like a vexed issue in terms of artist citations.

144
00:09:57,600 --> 00:10:02,700
So suppose you want to track who's being used in these artist names.

145
00:10:02,700 --> 00:10:06,140
Well, that's an entity recognition problem, right?

146
00:10:06,140 --> 00:10:09,060
It's also kind of hard for various reasons.

147
00:10:09,060 --> 00:10:13,500
I mean, right out of the box, it looks like it ought to be easy.

148
00:10:13,500 --> 00:10:15,800
But here's why it gets hard.

149
00:10:15,800 --> 00:10:22,140
So I took 80,000 stable diffusion prompts and the repo link is shown there.

150
00:10:22,140 --> 00:10:23,620
There's much bigger sets out there.

151
00:10:23,620 --> 00:10:28,140
But for the purpose of this talk, I didn't want to use a big set.

152
00:10:28,140 --> 00:10:36,700
Went through Spacey, a natural language processing library, and got these totals.

153
00:10:36,700 --> 00:10:44,500
So right away, you can see the ones in red are essentially the same artist.

154
00:10:44,500 --> 00:10:52,780
Art germ, art germ, Stanley, art germ, it would be probably in quotes there.

155
00:10:52,780 --> 00:10:58,020
So like this is one of those immediate questions that you hit immediately when you do one of

156
00:10:58,020 --> 00:11:02,100
these projects, which is what do we do about that?

157
00:11:02,100 --> 00:11:08,220
I want to point out for people who know about using databases of entities to disambiguate

158
00:11:08,220 --> 00:11:12,980
or to link, a lot of these artists aren't in Wikipedia.

159
00:11:12,980 --> 00:11:17,420
You have to go to other places to get any kind of canonical reference ID or page for

160
00:11:17,420 --> 00:11:19,700
them.

161
00:11:19,700 --> 00:11:22,220
So yeah.

162
00:11:22,220 --> 00:11:26,020
The thing that comes up quickly with this kind of project is what are you going to do

163
00:11:26,020 --> 00:11:27,020
with these?

164
00:11:27,020 --> 00:11:32,340
Like, what do you care about when you want to look for entities like that?

165
00:11:32,340 --> 00:11:41,340
So for instance, this is Mastodon's Spark lines showing trending terms, hashtags on

166
00:11:41,340 --> 00:11:43,020
one of the servers.

167
00:11:43,020 --> 00:11:49,180
So in this case, like, probably they don't care at Mastodon, but we might care from an

168
00:11:49,180 --> 00:11:50,860
NLP or data science perspective.

169
00:11:50,860 --> 00:11:53,340
Do we care that people don't know what the right term is?

170
00:11:53,340 --> 00:11:59,420
So for the natural language conference, EMNLP, do we care that people are using both EMNLP

171
00:11:59,420 --> 00:12:03,140
2022 and without the 2022?

172
00:12:03,140 --> 00:12:06,540
Should those be in some sense merged or not?

173
00:12:06,540 --> 00:12:10,060
So this is essentially an entity resolution problem.

174
00:12:10,060 --> 00:12:15,060
And the other question is, like, do you want to know the truth about what people are typing?

175
00:12:15,060 --> 00:12:18,940
Or do you want to know what, like, do you want to know what they're actually typing

176
00:12:18,940 --> 00:12:24,020
in terms of how they type it out, the string, including mistakes?

177
00:12:24,020 --> 00:12:26,060
Or how they refer to our term?

178
00:12:26,060 --> 00:12:28,940
Do they know that it's Stanley Lau?

179
00:12:28,940 --> 00:12:33,820
And so all these questions come up quickly about, like, what is truth and what are you

180
00:12:33,820 --> 00:12:37,660
going to be accurate with respect to in the data?

181
00:12:37,660 --> 00:12:41,900
You hit these problems immediately with this kind of project.

182
00:12:41,900 --> 00:12:46,400
So like I said, entity resolution is what we're talking about here, where we're usually

183
00:12:46,400 --> 00:12:53,020
talking about techniques that sort of identify, group, and link things to some object in the

184
00:12:53,020 --> 00:12:54,260
real world.

185
00:12:54,260 --> 00:12:59,460
So there is some artist who is Art Germ, and how do we talk about him?

186
00:12:59,460 --> 00:13:01,300
Or how do people talk about him?

187
00:13:01,300 --> 00:13:05,020
And do we care about the differences in how they talk about him or not?

188
00:13:05,020 --> 00:13:10,060
So that's where the UI and the sort of system design part of it comes in.

189
00:13:10,060 --> 00:13:18,060
This is a huge topic of research and of tooling and companies have solutions, et cetera.

190
00:13:18,060 --> 00:13:19,060
I'm a person.

191
00:13:19,060 --> 00:13:20,060
I'm not a company.

192
00:13:20,060 --> 00:13:21,060
Well, I am technically.

193
00:13:21,060 --> 00:13:23,060
But you know what I mean.

194
00:13:23,060 --> 00:13:28,060
So you end up doing research on, like, what are the best ways to deal with this?

195
00:13:28,060 --> 00:13:34,260
You've only got a certain number of hours or days for the client or whatever.

196
00:13:34,260 --> 00:13:38,780
There's a bunch of papers and links that you can follow on this.

197
00:13:38,780 --> 00:13:41,900
But some of the issues that you're going to come up with, you've done your named entity

198
00:13:41,900 --> 00:13:42,900
recognition.

199
00:13:42,900 --> 00:13:47,540
You've got in that data, you've got misspellings or orthographic variants that you found.

200
00:13:47,540 --> 00:13:51,700
Like Leonardo da Vinci might be written in any number of ways.

201
00:13:51,700 --> 00:13:57,580
There might be misspellings, like Leonardo da Vinci.

202
00:13:57,580 --> 00:14:01,220
Looks like probably they're trying to refer to the same person.

203
00:14:01,220 --> 00:14:04,100
Maybe not in some cases.

204
00:14:04,100 --> 00:14:07,460
And then we have things like name and label variants that are all true but are different.

205
00:14:07,460 --> 00:14:13,860
So this comes up a lot in researching things related to women's names.

206
00:14:13,860 --> 00:14:19,700
So Mrs. Smith, Mrs. John Smith, Pamela Smith, Mae Austin, these are all potentially the

207
00:14:19,700 --> 00:14:21,580
same person.

208
00:14:21,580 --> 00:14:24,780
They're referred to, unfortunately, with a man's name in some cases.

209
00:14:24,780 --> 00:14:31,100
So your entity resolution situation has to handle these, right?

210
00:14:31,100 --> 00:14:35,460
Then there's misrecognitions of the model.

211
00:14:35,460 --> 00:14:42,300
So for instance, Mary's, it's actually Mary that's the entity, but this particular spacing

212
00:14:42,300 --> 00:14:48,140
model I was using often had an apostrophe S merged in with the token.

213
00:14:48,140 --> 00:14:50,300
Coreference.

214
00:14:50,300 --> 00:14:55,660
So this is essentially another problem that sometimes clients care about and sometimes

215
00:14:55,660 --> 00:14:56,660
they don't.

216
00:14:56,660 --> 00:14:57,660
So Barry the bat was a fun guy.

217
00:14:57,660 --> 00:14:59,420
Barry partied all night.

218
00:14:59,420 --> 00:15:04,740
Barry the bat and Barry are referring to the same entity with different strings.

219
00:15:04,740 --> 00:15:10,140
So this is also classic with full names in journalism and then you use the last name

220
00:15:10,140 --> 00:15:13,420
to refer to somebody.

221
00:15:13,420 --> 00:15:18,580
So sometimes clients want those merged and sometimes they don't.

222
00:15:18,580 --> 00:15:21,620
You need a coreference detector to handle that.

223
00:15:21,620 --> 00:15:25,020
Or crappy heuristics, which is what I end up doing a lot.

224
00:15:25,020 --> 00:15:26,060
All right.

225
00:15:26,060 --> 00:15:27,660
So tooling to handle this.

226
00:15:27,660 --> 00:15:34,380
So if you're just interested in counting and merging things, there's a whole bunch of tools

227
00:15:34,380 --> 00:15:36,420
out there that I've used a lot.

228
00:15:36,420 --> 00:15:42,660
One of them is the string grouper tool, which has various algorithms for how it tries to

229
00:15:42,660 --> 00:15:45,900
merge strings that might be the same.

230
00:15:45,900 --> 00:15:48,980
So it's a cool repo.

231
00:15:48,980 --> 00:15:51,100
I recommend looking at it.

232
00:15:51,100 --> 00:15:57,220
And you can do things like get what the similarity threshold is, use different algorithms, et

233
00:15:57,220 --> 00:15:58,220
cetera.

234
00:15:58,220 --> 00:16:04,700
So what essentially you're seeing here is I've run this thing over my list of artists

235
00:16:04,700 --> 00:16:07,820
or names from that data.

236
00:16:07,820 --> 00:16:11,660
And on the left is the original string and on the right it says right name.

237
00:16:11,660 --> 00:16:16,380
That's the first mention in the group that it made that it's saying these are all probably

238
00:16:16,380 --> 00:16:17,380
the same thing.

239
00:16:17,380 --> 00:16:23,700
So you can see down at the bottom, Krenz Koshart, et cetera, et cetera, et cetera.

240
00:16:23,700 --> 00:16:26,380
They're all basically the same.

241
00:16:26,380 --> 00:16:31,740
Now you're going to find that you have errors in your data.

242
00:16:31,740 --> 00:16:34,780
So those are the apostrophe S's, et cetera.

243
00:16:34,780 --> 00:16:35,780
Some of them are just bad.

244
00:16:35,780 --> 00:16:39,420
You might want to use a different tool.

245
00:16:39,420 --> 00:16:44,900
So here we're looking at like a combined list of who are the biggest groups.

246
00:16:44,900 --> 00:16:52,540
So Greg Rutkowski, Franz Puka had the biggest number of different variants, et cetera.

247
00:16:52,540 --> 00:16:56,420
Notice that not all of these are artists, some of them are content of the image like

248
00:16:56,420 --> 00:16:57,900
Emma Watson.

249
00:16:57,900 --> 00:17:03,140
So do you care is the question.

250
00:17:03,140 --> 00:17:07,100
So some of these things also have to be done by hand.

251
00:17:07,100 --> 00:17:12,500
So for instance, for big string differences, things like Price Waterhouse Cooper, PwC is

252
00:17:12,500 --> 00:17:14,540
a valid way of referring to it.

253
00:17:14,540 --> 00:17:19,740
So you have to actually create a dictionary that says this is a thing that also is this

254
00:17:19,740 --> 00:17:21,980
thing.

255
00:17:21,980 --> 00:17:26,020
So the by hand thing is super vexed.

256
00:17:26,020 --> 00:17:27,580
You have to do this a lot, it turns out.

257
00:17:27,580 --> 00:17:30,860
So James Jean and James Dean are not the same person.

258
00:17:30,860 --> 00:17:33,180
One is an actor and one is an artist.

259
00:17:33,180 --> 00:17:38,460
But string similarity would say that they should be grouped together and you're going

260
00:17:38,460 --> 00:17:40,100
to have to manage that.

261
00:17:40,100 --> 00:17:47,420
So you end up doing a ton of by hand stuff and maintaining databases and sort of lists

262
00:17:47,420 --> 00:17:48,780
of entities.

263
00:17:48,780 --> 00:17:54,260
It's just a problem in this space that you have to do that.

264
00:17:54,260 --> 00:17:57,860
And the tooling around this isn't superb.

265
00:17:57,860 --> 00:18:03,220
I just end up in dictionaries and lists a lot.

266
00:18:03,220 --> 00:18:04,860
It's a big problem.

267
00:18:04,860 --> 00:18:08,340
One of the I'm going to skip this and move on to the next example.

268
00:18:08,340 --> 00:18:13,700
So one of the tools that I use actually, this is very norm conf, is OpenRefine.

269
00:18:13,700 --> 00:18:19,940
So this is a GUI tool for helping you deal with data problems and sort of merge them

270
00:18:19,940 --> 00:18:22,180
and manage them.

271
00:18:22,180 --> 00:18:28,660
This is a great tool because I can just decide in this label, like right there in the list,

272
00:18:28,660 --> 00:18:35,020
how do I want to call that group and fix the string labeling issues.

273
00:18:35,020 --> 00:18:36,700
It requires a number of steps.

274
00:18:36,700 --> 00:18:41,620
And essentially what you want to do is you do this cluster and edit thing.

275
00:18:41,620 --> 00:18:47,380
And I recommend a sort of a workflow where you keep the original entity over in another

276
00:18:47,380 --> 00:18:51,580
column and then you work off this column and then you end up with a mapping so that you

277
00:18:51,580 --> 00:18:55,540
can use them later in a tool.

278
00:18:55,540 --> 00:18:56,540
All right.

279
00:18:56,540 --> 00:18:57,540
So wow.

280
00:18:57,540 --> 00:18:59,140
I'm definitely not going to get through all my slides.

281
00:18:59,140 --> 00:19:01,500
I knew this was coming.

282
00:19:01,500 --> 00:19:07,620
So basically you'll be able to manage a dictionary of the original and the merged and export

283
00:19:07,620 --> 00:19:10,220
the data and deal with it.

284
00:19:10,220 --> 00:19:16,260
One of the things, one of the reasons I want to talk about that is because dealing with

285
00:19:16,260 --> 00:19:21,740
rules is actually a big thing in NLP, at least in my world of NLP.

286
00:19:21,740 --> 00:19:25,740
You end up having to augment models with rules quite a lot.

287
00:19:25,740 --> 00:19:32,520
So, so, Spacy has a bunch of tools for helping you deal with this because you can write patterns

288
00:19:32,520 --> 00:19:38,140
and define rules that you can use as part of your pipeline in Spacy, which is super

289
00:19:38,140 --> 00:19:40,020
useful.

290
00:19:40,020 --> 00:19:46,340
So the entity ruler in particular allows you to say that this art germ version is also

291
00:19:46,340 --> 00:19:50,300
this art germ version with an ID.

292
00:19:50,300 --> 00:19:55,700
There's a whole bunch of docs and tooling around this that you can look at.

293
00:19:55,700 --> 00:20:01,460
So I'm now going to switch to this body prompts issue and the not safe for work classification

294
00:20:01,460 --> 00:20:04,560
problem that I was working on for one client.

295
00:20:04,560 --> 00:20:09,340
So say you have 8 million unlabeled examples and you're trying to work on a not safe for

296
00:20:09,340 --> 00:20:13,740
work porn classifier for these prompts.

297
00:20:13,740 --> 00:20:19,620
It turns out that one of the things that some of the moderators dealing with this data were

298
00:20:19,620 --> 00:20:23,020
looking at was mentions of porn stars.

299
00:20:23,020 --> 00:20:25,700
It turned out that that was actually not a bad idea.

300
00:20:25,700 --> 00:20:31,420
And in fact, in a lot of the cleaned up repos of this kind of data, the porn stars are still

301
00:20:31,420 --> 00:20:37,700
in there because nobody thought of ruling them out as content problems.

302
00:20:37,700 --> 00:20:40,660
So for instance, in this data set I was looking at.

303
00:20:40,660 --> 00:20:42,860
And let's talk about why that is.

304
00:20:42,860 --> 00:20:49,660
So the text itself is not bad, but the output image will probably be bad because it's been

305
00:20:49,660 --> 00:20:52,660
trained on photos of porn stars.

306
00:20:52,660 --> 00:20:56,620
So this is like, I was like, okay, this is an obvious gazetteer problem.

307
00:20:56,620 --> 00:20:59,020
I just need a list of porn stars.

308
00:20:59,020 --> 00:21:01,460
Where am I going to get a list of porn stars?

309
00:21:01,460 --> 00:21:04,660
Well, it turns out Wikipedia is excellent at this.

310
00:21:04,660 --> 00:21:05,660
Why might that be?

311
00:21:05,660 --> 00:21:11,380
Well, that's because an awful lot of editors of Wikipedia are white men.

312
00:21:11,380 --> 00:21:15,140
And so it's just a known fact that the Androzon women porn stars are amazing.

313
00:21:15,140 --> 00:21:20,140
So this turns out to be a very good database for getting lists of porn stars.

314
00:21:20,140 --> 00:21:23,780
So Chris Alvin is talking later.

315
00:21:23,780 --> 00:21:26,860
Someone give him shit about this.

316
00:21:26,860 --> 00:21:32,900
So what I did, and this is something that a tool like Spacey will allow you to do, is

317
00:21:32,900 --> 00:21:39,260
you basically take a classifier that doesn't necessarily know enough about, say, porn stars,

318
00:21:39,260 --> 00:21:44,500
and you add them in as rules because you have this ability to do rules, and then you basically

319
00:21:44,500 --> 00:21:49,820
can override the text classification in case it doesn't know about the porn star and say,

320
00:21:49,820 --> 00:21:53,540
this is actually bad stuff.

321
00:21:53,540 --> 00:21:58,100
So I've spent a bunch of time doing, like, these creative modifications of pipelines

322
00:21:58,100 --> 00:22:00,760
in Spacey, and this is one of them.

323
00:22:00,760 --> 00:22:03,800
So I'm here to tell you how to do that.

324
00:22:03,800 --> 00:22:09,620
So basically all you have to do is define your own component that says, if I have a

325
00:22:09,620 --> 00:22:16,840
label that's a porn person, I'm going to call that a positive classification for porn, regardless

326
00:22:16,840 --> 00:22:18,820
of what the model did.

327
00:22:18,820 --> 00:22:20,020
And this is the crucial thing.

328
00:22:20,020 --> 00:22:25,500
It's like override the model and just don't allow it in this case.

329
00:22:25,500 --> 00:22:31,340
So you can get the code from my repo or from the slides, but basically we load in a trained

330
00:22:31,340 --> 00:22:39,300
text cat model, and then we define our rules for all the porn people, and we say override

331
00:22:39,300 --> 00:22:42,820
and use those rules for the porn person instead.

332
00:22:42,820 --> 00:22:44,420
This is just tremendously useful.

333
00:22:44,420 --> 00:22:50,100
So for instance, I can have coffee with Mia Khalifa in the coffee shop and come out with

334
00:22:50,100 --> 00:22:51,420
these entities.

335
00:22:51,420 --> 00:22:55,660
One of them is a porn person, so the classification is definitely, like, this is not safe for

336
00:22:55,660 --> 00:22:56,660
work.

337
00:22:56,660 --> 00:23:01,700
Whereas, otherwise, if it's not a porn person, I get sort of ordinary entities, and then

338
00:23:01,700 --> 00:23:04,300
I'm fine.

339
00:23:04,300 --> 00:23:09,500
I've learned a lot about doing really, really wacky things with Spacey pipelines, some of

340
00:23:09,500 --> 00:23:15,380
which would not be probably approved of by the Spacey explosion team, but basically here

341
00:23:15,380 --> 00:23:16,380
we are.

342
00:23:16,380 --> 00:23:21,740
So a bunch of examples in my code in the repo.

343
00:23:21,740 --> 00:23:23,180
And I just want to wrap up.

344
00:23:23,180 --> 00:23:30,060
So Peter Baumgartner, who's also speaking later, has a great post about rules in NLP.

345
00:23:30,060 --> 00:23:35,020
And I mean, it's essentially the process you kind of want to go through is sort of this,

346
00:23:35,020 --> 00:23:41,500
which is you use rules to help yourself train a model, and then you review your errors,

347
00:23:41,500 --> 00:23:43,580
you fix and improve your model.

348
00:23:43,580 --> 00:23:47,940
This is fantastic, but also you might want to keep the rules around.

349
00:23:47,940 --> 00:23:52,980
So in this case, I mean, rules are fragile, and in this kind of context, you probably

350
00:23:52,980 --> 00:24:00,700
want to have, like, absolutely call it porn or call it not safe for work if this name

351
00:24:00,700 --> 00:24:01,700
comes up.

352
00:24:01,700 --> 00:24:05,580
You don't want to allow the model to be unsure.

353
00:24:05,580 --> 00:24:09,180
You just want to say this is blocked.

354
00:24:09,180 --> 00:24:11,660
There are a ton of Spacey resources.

355
00:24:11,660 --> 00:24:14,900
I have become an expert on digging through them.

356
00:24:14,900 --> 00:24:20,300
You definitely want to use my list if you aren't up to date with how to use Spacey and

357
00:24:20,300 --> 00:24:22,620
how to learn among their tools.

358
00:24:22,620 --> 00:24:25,820
There's a lot of information sort of floating around.

359
00:24:25,820 --> 00:24:30,300
There's a giant scary flow chart, which I'll show you a glimpse of in a second, but it's

360
00:24:30,300 --> 00:24:31,780
very, very useful.

361
00:24:31,780 --> 00:24:36,940
They have a customized labeling tool, and essentially they're like great people, and

362
00:24:36,940 --> 00:24:38,260
two of them are speaking here.

363
00:24:38,260 --> 00:24:41,360
So obviously they're excellent.

364
00:24:41,360 --> 00:24:48,060
This is only a part of the giant scary flow chart, but it is tremendously useful.

365
00:24:48,060 --> 00:24:50,580
Sorry to jump in, Lynn.

366
00:24:50,580 --> 00:24:52,940
It would be a shame if we didn't get to any questions.

367
00:24:52,940 --> 00:24:54,820
We've got a real quick one for you.

368
00:24:54,820 --> 00:24:57,500
Have you found that there are some sentence embedding models that work better out of the

369
00:24:57,500 --> 00:25:01,700
box for the purposes of UMAT?

370
00:25:01,700 --> 00:25:07,820
So I just try a bunch, because like I said, it's an exploratory method for me, so I just

371
00:25:07,820 --> 00:25:10,500
try a few until I see if I can see clusters.

372
00:25:10,500 --> 00:25:16,260
But the problem is that with UMAP, you also have a bunch of different things you can control

373
00:25:16,260 --> 00:25:17,440
in the settings.

374
00:25:17,440 --> 00:25:40,920
So you can end up iterating a lot until you get to the point that you like your picture.

