1
00:00:00,000 --> 00:00:10,200
My name is Helena Seren. I live in New Jersey. I'm a ceramic artist and I live with my rescue

2
00:00:10,200 --> 00:00:19,280
dog, Cumin, my three GPUs, and my husband. And today I'll be talking about game of construction.

3
00:00:19,280 --> 00:00:27,000
Construction is a metaphor for art making by Pollock and he famously said, it is all

4
00:00:27,000 --> 00:00:33,200
a game of construction. Some with a brush, some with a shovel, some chose a pen, and

5
00:00:33,200 --> 00:00:40,040
some of us choose neural networks to do our art. But the first question, what makes me

6
00:00:40,040 --> 00:00:49,600
a ceramic artist now qualified for talking at a male conference? I will say that recently,

7
00:00:49,600 --> 00:00:58,720
not recently, a couple of years ago, I discovered this magical word, Kugei. And it means craft

8
00:00:58,720 --> 00:01:06,280
in Japanese, but literally it means engineered art. And since then I'm calling myself engineering

9
00:01:06,280 --> 00:01:14,380
artist. And as a Kugei practitioner, it was my whole life when I was coding the low level

10
00:01:14,380 --> 00:01:23,560
protocols in telecom, when I kind of expanded into machine learning seven years ago, and

11
00:01:23,560 --> 00:01:32,460
when I'm now a practicing artist making art for a living for the last almost four years.

12
00:01:32,460 --> 00:01:39,520
So Kugei is our word here. So, and I'll be talking about creating art, making art in

13
00:01:39,520 --> 00:01:46,200
the intersection of art and technology. So technology is our tool for us engineering

14
00:01:46,200 --> 00:01:55,320
artists. And it's not only with AI, it's basically for the last maybe century, it's even accelerated

15
00:01:55,320 --> 00:02:02,700
where technical invention brought the new art form. So we can pinpoint this in like

16
00:02:02,700 --> 00:02:13,120
history of art is like prints, then photography, cinema, computer arts, and now AI kind of

17
00:02:13,120 --> 00:02:22,800
like burst into the scene big time. So I work with generative adversarial networks. I mean,

18
00:02:22,800 --> 00:02:30,380
up until prompt-based AI came into the picture, I would say like AI interchangeably with GANs.

19
00:02:30,380 --> 00:02:36,760
Now I need to qualify that I work with GANs mostly. So the base recipe, I work, first

20
00:02:36,760 --> 00:02:44,280
of all, I work with my own data sets. I collect them. I make photography or I use my digitized

21
00:02:44,280 --> 00:02:52,720
sketches. So here is like my famous vegan model, which I took like salad leaves at the

22
00:02:52,720 --> 00:03:00,360
beginning of COVID basically sitting at home in lockdown. So I took, like I made two salad

23
00:03:00,360 --> 00:03:07,840
leaves, 2000 pictures of lettuce in like on my force for two days, making pictures in

24
00:03:07,840 --> 00:03:19,280
the light box of few boxes of whole food, organic, clean lettuce leaves. And I trained

25
00:03:19,280 --> 00:03:28,560
the network StyleGAN and it started after like few days of training, it started generating

26
00:03:28,560 --> 00:03:36,680
this, creating this generative produce, what I started calling after that, like Entusys.

27
00:03:36,680 --> 00:03:46,360
So this talk, I'm quoting a lot of, is a culture trash and I'm like always into quoting. And

28
00:03:46,360 --> 00:03:54,880
I think this quote by Carpate is really kind of like relevant for artist engineers who

29
00:03:54,880 --> 00:04:03,040
like team caring. So basically training your network is you sitting through the like days

30
00:04:03,040 --> 00:04:11,000
and nights watching your model kind of making stuff, saving checkpoints. And the difference

31
00:04:11,000 --> 00:04:21,960
may be with like researchers or scientists who work with neural networks. My loss functions

32
00:04:21,960 --> 00:04:27,200
are more kind of like wandering functions. I mean, I don't kind of care for particular

33
00:04:27,200 --> 00:04:34,920
loss. I care more about visuals and that's why I'm glued to the screen because I need

34
00:04:34,920 --> 00:04:43,400
to see like particular checkpoints where I kind of discover some gem. And I used to call

35
00:04:43,400 --> 00:04:50,720
training networks is my meditative regime, breathing in and out with the loss function.

36
00:04:50,720 --> 00:04:59,280
So but basically it's like a game of curation. I mean, a lot of this and for practitioners

37
00:04:59,280 --> 00:05:05,080
being them engineers or artists is just like you curating your results. And that's why

38
00:05:05,080 --> 00:05:14,360
I have this famous Burrows quote because you need, I mean, in any art form, you kind of

39
00:05:14,360 --> 00:05:20,560
like do sketches, you do a lot of curation. It's not kind of like particular to making

40
00:05:20,560 --> 00:05:27,960
art with the eye. But also I think it's like a great tool of honing your visual perception.

41
00:05:27,960 --> 00:05:34,480
I mean, I kind of like some of my early work is a cringe, but some of them is sort of like

42
00:05:34,480 --> 00:05:42,680
you know, looking at the world like a child and spoiled and in awe of what you see. And

43
00:05:42,680 --> 00:05:50,440
this is one of my basically one of my first publicly shared pieces, which I'm still very

44
00:05:50,440 --> 00:05:59,160
proud about. So game of subversion. I keep saying that, like, in a sense, us working

45
00:05:59,160 --> 00:06:06,960
with the same networks that researchers work, it's a little bit kind of like subverting

46
00:06:06,960 --> 00:06:15,560
what the model do. Because if researchers, their goal at large, getting to some digital

47
00:06:15,560 --> 00:06:23,520
correctness, in my case, is more like seeing interesting and novel and exciting stuff.

48
00:06:23,520 --> 00:06:29,320
So that's why maybe nobody cares about like first cycles of training. But for me, and

49
00:06:29,320 --> 00:06:36,800
this is a picture on the left, the fleeting beauty of early iterations. But also like

50
00:06:36,800 --> 00:06:42,840
when you get to like beyond the expiration date, I mean, I have some models on the right

51
00:06:42,840 --> 00:06:49,240
you see the example of model drop, of mode drop, where I trained basically the network

52
00:06:49,240 --> 00:06:55,200
for almost like two years on and off. Yeah, and this is the network that was trained on

53
00:06:55,200 --> 00:07:02,520
my sketches. But also one of the technique I used a lot after I kind of like passed this

54
00:07:02,520 --> 00:07:08,880
excitement of like something in something out, I started building the pipelines. So

55
00:07:08,880 --> 00:07:15,960
kind of like one of the example of like poor woman super resolution where I sort of like

56
00:07:15,960 --> 00:07:23,680
trained the next network or the result of the previous one, just getting to the resolution

57
00:07:23,680 --> 00:07:33,600
which I wanted or needed because like the initial result was like 256 by 256. So I love

58
00:07:33,600 --> 00:07:39,480
this quote by Carpati, who said, it is not a single model, it's a beautiful composition

59
00:07:39,480 --> 00:07:45,800
of many sub models. That's how my pipelines look because I have like about 100 models

60
00:07:45,800 --> 00:07:56,440
that I use practically all the time. Another thing that happens is like you make art in

61
00:07:56,440 --> 00:08:03,880
on the limitations of your medium. So early on, like again, to overcome this problem of

62
00:08:03,880 --> 00:08:08,960
low resolution, I started working with the grid. And for me, it became like a neural

63
00:08:08,960 --> 00:08:15,640
aesthetics in a sense. So I could have like interesting images and augment them or kind

64
00:08:15,640 --> 00:08:23,160
of like make them bigger by employing the grid approach. And then I was surprised where

65
00:08:23,160 --> 00:08:30,120
like famous artist Jack Whitten called this the grid is a DNA of visual perception. It's

66
00:08:30,120 --> 00:08:36,600
an amazing quote. I didn't think in this terms, but that's what it is. Another thing like

67
00:08:36,600 --> 00:08:44,240
again, like playing this subversion game, people who worked with neural networks early

68
00:08:44,240 --> 00:08:50,000
on remember this like problem of checkerboard. So kind of I jokingly started calling this

69
00:08:50,000 --> 00:08:56,320
checkerboard amplification series. So it's kind of like post-genism. I started writing

70
00:08:56,320 --> 00:09:04,120
a lot of like Python scripts, just doing like this computational grids playing with a few

71
00:09:04,120 --> 00:09:10,320
kind of like digital collaging and stuff. So that's how this series of non-pintilism

72
00:09:10,320 --> 00:09:17,960
and checkerboard amplification series were born. Another thing like for a couple of years,

73
00:09:17,960 --> 00:09:24,960
I worked with my sketches and it's resulted in this, what I call carvings of Latinsco,

74
00:09:24,960 --> 00:09:37,240
riffing on the Lascaux caves where like caveman's like our predecessors in a sense drew on the

75
00:09:37,240 --> 00:09:46,240
cave. So I think like we early AI practitioners drew our part on this Latinsco caves. But

76
00:09:46,240 --> 00:09:53,240
also I like this quote by Linda Berry who said like take advantage of basic human inclination

77
00:09:53,240 --> 00:10:02,440
to find meaning in random information. So basically the semi abstract output of the

78
00:10:02,440 --> 00:10:10,200
early models invited this kind of like planning and gameplay. And I worked with it a lot.

79
00:10:10,200 --> 00:10:17,680
I mean, I kind of like, it's sort of like honing your verbal perception or your kind

80
00:10:17,680 --> 00:10:26,640
of like pun mechanism. This is kind of like controversial, but for me it's very important.

81
00:10:26,640 --> 00:10:35,080
I kind of like burnt a lot on this. So when like almost everybody, I started with AWS

82
00:10:35,080 --> 00:10:41,000
of course, but then when I like five, six years ago started training on my own art,

83
00:10:41,000 --> 00:10:49,240
I decided I need my own setup. Not only because I didn't want to move like my data all the

84
00:10:49,240 --> 00:10:57,800
time back and forth and forgetting to disconnect the volumes. I decided I basically want my

85
00:10:57,800 --> 00:11:07,080
own rig. And I started with the GTX, GeForce like 1060, I think. And then kind of people

86
00:11:07,080 --> 00:11:16,760
of Nvidia gifted me with this quadro. And like I say, GPU is a girl best friend, of

87
00:11:16,760 --> 00:11:24,960
course. So now I have like this small stable of GPUs. And like, since I sort of like make

88
00:11:24,960 --> 00:11:32,760
this phrase in the different applied arts, I last year I kind of got my own kiln and

89
00:11:32,760 --> 00:11:38,280
I have like other kinds of stuff. And it's not about bragging. I mean, it's like printing.

90
00:11:38,280 --> 00:11:44,800
I used to print a lot and like outsourcing, it's a pain and you pay dear because first

91
00:11:44,800 --> 00:11:49,720
of all, in quality, because you can't control this. And another kind of like you pay money

92
00:11:49,720 --> 00:11:56,440
for food to people which you can kind of like not pay and make your art more affordable

93
00:11:56,440 --> 00:12:06,320
in a sense. So this was my star slide in every talks for the last like three years. And it

94
00:12:06,320 --> 00:12:14,240
sort of became obsolete with like prompt models and I'll explain why. So in a nutshell, what

95
00:12:14,240 --> 00:12:20,560
I was talking about here and it was relevant when like for training on your own data. So

96
00:12:20,560 --> 00:12:28,120
the more you own and the more you curate your data, the more original you are. So like in

97
00:12:28,120 --> 00:12:36,560
the origin of this chart, you have the less original art. So it's like art breather and

98
00:12:36,560 --> 00:12:48,040
first kind of like public trained applications. But then like prompt text to image applications

99
00:12:48,040 --> 00:12:56,160
came into for this year and we all basically gathered around this push the button thing.

100
00:12:56,160 --> 00:13:02,080
And this is like I thought I looked at this slide and thought like maybe it's not relevant

101
00:13:02,080 --> 00:13:13,480
anymore. And then I think this Z-axis with ideas and I think like since when you can

102
00:13:13,480 --> 00:13:21,200
find them, they join them. So we are all in the realms of this prompt models. I mean,

103
00:13:21,200 --> 00:13:25,840
it's inevitable and this is something the reality that we can't deny. So I think it's

104
00:13:25,840 --> 00:13:32,800
still kind of like relevant graph, but with more access, how ideas become more and more

105
00:13:32,800 --> 00:13:43,120
important. So that's probably again, I mean, something that take away from here. So to

106
00:13:43,120 --> 00:13:52,280
finish this part is just how I see this AI artist or basically any artist who works with

107
00:13:52,280 --> 00:14:00,240
like technology. So you're basically curiosity driven. I mean, it's like the world of what

108
00:14:00,240 --> 00:14:06,360
ifs you try this. I mean, what if I kind of like make this data set or this data set?

109
00:14:06,360 --> 00:14:13,160
You improvise on the spot. You see your model behave this way. You kind of like encourage

110
00:14:13,160 --> 00:14:20,160
it to do it or kind of like do something else. You are very attentive and good listener because

111
00:14:20,160 --> 00:14:26,400
I think like patience is a very important trait for working with technology. You need

112
00:14:26,400 --> 00:14:33,080
to be attuned to what you do, but you also need to like tinkering. It's a lot of tinkering

113
00:14:33,080 --> 00:14:39,600
when you write your code, whatever like AI or any other thing. So creative coder goes

114
00:14:39,600 --> 00:14:47,120
without saying. You make your own tools. I mean, the less applications you use, the more

115
00:14:47,120 --> 00:14:52,560
interesting you are. It's not guaranteed, but still kind of like a precursor. So collector

116
00:14:52,560 --> 00:14:59,440
process oriented, we already talked about this meditative qualities of AI training.

117
00:14:59,440 --> 00:15:05,960
Collector, I mean, I basically save everything. I mean, you never know. I mean, whatever,

118
00:15:05,960 --> 00:15:13,600
checkpoints, like outputs, you never know what you can use. I used to kind of like pre-curate

119
00:15:13,600 --> 00:15:19,000
them. Then I stopped doing it because something that looked like a junk two years ago, all

120
00:15:19,000 --> 00:15:25,200
of a sudden becomes trash right now. So you never know your perception changes, the tools

121
00:15:25,200 --> 00:15:34,280
you're working with changes. So a lot of like dynamics. Also easily excited and easily bored.

122
00:15:34,280 --> 00:15:41,000
I mean, because we work with novelty tools, you move around a lot. So you get excited,

123
00:15:41,000 --> 00:15:47,600
you try it. I mean, you maybe need to stay with certain kind of like stuff for a while

124
00:15:47,600 --> 00:15:54,200
to see. So you're not jumping from thing to thing like without giving it a chance to express

125
00:15:54,200 --> 00:16:01,960
itself. And also I think it's very important not to be precious about your work. And of

126
00:16:01,960 --> 00:16:06,920
course, having a good sense of humor helps. I mean, because it kind of like helps with

127
00:16:06,920 --> 00:16:14,480
the frustration, but also kind of like because AI is still kind of goofy often. So in the

128
00:16:14,480 --> 00:16:22,000
next like five minutes that left, I'll talk about projects that I worked at. And I mean,

129
00:16:22,000 --> 00:16:29,320
neural bricolage manifest, like I keep asking, getting this question, what bricolage means.

130
00:16:29,320 --> 00:16:37,880
And neural is kind of like comes from neural networks, but bricolage is basically making

131
00:16:37,880 --> 00:16:44,040
useful and beautiful out of heap of latent space. I mean, this is how I kind of like

132
00:16:44,040 --> 00:16:53,520
riff on the definition. And my art is for people. So again, like artists, some artists

133
00:16:53,520 --> 00:16:59,840
say, oh, I will create because it's like I create for myself. There is a part of this,

134
00:16:59,840 --> 00:17:06,080
but they make stuff for people. Without people, I would be just curl in the armchair and just

135
00:17:06,080 --> 00:17:17,440
reading stuff. So a little bit of like, this is like mandatory milestone line. And how

136
00:17:17,440 --> 00:17:24,400
I call myself like taking this expression from my friend. So I basically awkwardly stuck

137
00:17:24,400 --> 00:17:29,560
between digital and analog. In fact, I was very surprised when somebody like three years

138
00:17:29,560 --> 00:17:35,240
ago called me a digital artist. I never consider myself being it. Digital was always means

139
00:17:35,240 --> 00:17:42,880
to an end and then being like projects I discuss right now. So I mean, I try to kind of like

140
00:17:42,880 --> 00:17:50,960
do interesting stuff, different stuff with the output. So I went through the period of

141
00:17:50,960 --> 00:17:58,720
printmaking, silkscreen, cyanotype. Then I started making the artist books because it's

142
00:17:58,720 --> 00:18:03,480
so kind of like I love books and I think it's like everybody needs to make at least like

143
00:18:03,480 --> 00:18:12,040
one book in their lives. So the first book was like a religious trope on like this Genesis

144
00:18:12,040 --> 00:18:19,440
play. And it was like small, all my books are self-published. So this was like my first

145
00:18:19,440 --> 00:18:26,600
books with the latent doodles. Then I think it's very important to have like collaborations

146
00:18:26,600 --> 00:18:33,960
with like artists. I'm very independent, but still I believe like when you find the great

147
00:18:33,960 --> 00:18:41,160
collaborator is just like amazing. So in this case, Dmitry Chernyak, a generative artist

148
00:18:41,160 --> 00:18:50,400
created for me the training data sets with 3GS. And then I trained like 17 models total

149
00:18:50,400 --> 00:18:55,440
and so like from scratch. And that's like on the right, the results that we are publishing

150
00:18:55,440 --> 00:19:03,320
the book is coming up soon. So another kind of collaboration is the book of vegan was

151
00:19:03,320 --> 00:19:09,520
just published with the amazing food writer, Kate Ray, who wrote recipes for this book.

152
00:19:09,520 --> 00:19:15,840
I'm very proud. I designed this book from scratch. It's like I went overboard. It's

153
00:19:15,840 --> 00:19:24,120
150 pages thick, but still this is what it is. And like we discussed, I'm in the pottery

154
00:19:24,120 --> 00:19:33,640
for this year. And my first foray with generative pottery was like a couple of years ago. And

155
00:19:33,640 --> 00:19:42,320
I liked this like riff on pottery baron, which became pottery again. And yeah, I mean, then

156
00:19:42,320 --> 00:19:49,880
I kind of like stumble on a learning curve because it's a slow process. I'm kind of like

157
00:19:49,880 --> 00:19:58,280
all over the place person. So ceramics was not that, I mean, it was tough. But again,

158
00:19:58,280 --> 00:20:07,400
COVID and stuff sort of slowed me down. And I think like this time I'm ready. And basically

159
00:20:07,400 --> 00:20:15,880
I have my first fruits of my kiln coming out. And again, in a nutshell, it's something that

160
00:20:15,880 --> 00:20:27,960
I use again outputs to kind of like go through the convoluted process of printing 3D, you

161
00:20:27,960 --> 00:20:35,160
know, molds, then pressing or slip casting. So it's like very involved process. And maybe

162
00:20:35,160 --> 00:20:44,400
it's such a difference from like fast paced TIR that I really kind of like getting very

163
00:20:44,400 --> 00:21:03,360
used to it. And I love this. But to finish it is all a game of construction. Thank you.

