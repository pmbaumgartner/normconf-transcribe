1
00:00:00,000 --> 00:00:08,000
So in the spirit of NormCop, I thought that we could kick off with a good vintage tweet, which happens to be a favorite of mine.

2
00:00:08,000 --> 00:00:15,000
A wise man once pointed out that when you find yourself doing the same thing multiple times, you should move toward automation.

3
00:00:15,000 --> 00:00:23,000
I love Dave's second point here, which is when you find yourself answering the same questions, you should consider automating your advice.

4
00:00:23,000 --> 00:00:32,000
Today I want to talk about frameworks for efficiently conquering the unknowns that stand between us and getting things to production.

5
00:00:32,000 --> 00:00:43,000
So these are things that I've seen happen over and over again, and I'm trying to come up with some ways to combat some of the unknowns as we're starting new projects.

6
00:00:43,000 --> 00:00:49,000
For example, taking a machine learning model from idea to production often involves similar steps.

7
00:00:49,000 --> 00:00:53,000
So the data and the specific application might vary from project to project.

8
00:00:53,000 --> 00:00:59,000
But I know that at some point I'll have to get out, get some data, spin up a notebook and build something.

9
00:00:59,000 --> 00:01:11,000
And I can't fully automate that process, but I can find patterns in some of the thinking that drives each step and try to get more efficient that way by focusing on some of the patterns I'm seeing.

10
00:01:11,000 --> 00:01:17,000
The best way I've found to automate things that don't fit into a function or a package is via frameworks.

11
00:01:17,000 --> 00:01:19,000
And so that's why I want to talk about frameworks today.

12
00:01:19,000 --> 00:01:28,000
Frameworks are especially helpful for automating workflows around organizing my thinking and working through problems.

13
00:01:28,000 --> 00:01:33,000
They're boring, but they work really well and they make my life easier because I don't have to remember things.

14
00:01:33,000 --> 00:01:38,000
And so I feel like they're very norm-conf.

15
00:01:38,000 --> 00:01:49,000
I'm at Figma now, but previously worked at Online Med Ed, which is a medical education company, and they created hundreds of online videos on various topics to advance a flipped classroom model.

16
00:01:49,000 --> 00:02:00,000
In order to scale, we had to match our curriculum to that of med schools and governing bodies to make sure that our material covered all of the same points that they expected us to cover.

17
00:02:00,000 --> 00:02:11,000
This was a really fun problem because as Tolstoy foretold, each unique syllabus is unique in its own way, which in this case means that lessons are named very differently.

18
00:02:11,000 --> 00:02:20,000
They're parceled out differently in terms of how they're covered across different syllabi, and they definitely didn't match the lesson names that we had in our training data.

19
00:02:20,000 --> 00:02:32,000
So the manual version of matching syllabi with our curriculum involved a very knowledgeable medical subject matter expert reading a lesson and some of its key points on the syllabus,

20
00:02:32,000 --> 00:02:39,000
then searching on our site, which included keywords but not like the full text of all of the videos,

21
00:02:39,000 --> 00:02:49,000
and scrubbing through those videos that matched to see if it was really a match and how fully it covered each of the lesson sub points.

22
00:02:49,000 --> 00:02:58,000
You can imagine this is not ideal, and so we thought that maybe automating this matching process via machine learning would be really helpful.

23
00:02:58,000 --> 00:03:09,000
So here we've outlined this tool that would give the person the top end lessons to check first based on looking at the full content of each of the videos.

24
00:03:09,000 --> 00:03:14,000
And so this would significantly improve our ability to scale the process.

25
00:03:14,000 --> 00:03:17,000
And so this is what we proposed.

26
00:03:17,000 --> 00:03:25,000
This was the first machine learning model that we built at Online Med Ed, so there was a ton of uncertainty around the project.

27
00:03:25,000 --> 00:03:33,000
The biggest open question being like not knowing whether the type of classification we wanted to do would even work with our data,

28
00:03:33,000 --> 00:03:41,000
like was our data big enough and unique enough at the lesson level to actually do classification was a very open, important question.

29
00:03:41,000 --> 00:03:52,000
At the time, we were also doing a big rewrite of our code base, so there were also questions around where the model would eventually live and dependencies and whether we would be able to roll it out.

30
00:03:52,000 --> 00:03:59,000
Anything that I could do to reduce uncertainty, both for stakeholders and for myself, was really helpful here.

31
00:03:59,000 --> 00:04:08,000
So I wrote a lot of documentation around like inputs and outputs and what those would look like, what the architecture could look like, in order to build buy-in.

32
00:04:08,000 --> 00:04:14,000
Once we had buy-in, we decided that the best course of action would be to build a proof of concept.

33
00:04:14,000 --> 00:04:25,000
And so I decided to try a couple of frameworks out in that process to see if we could do it like quickly than I had, more quickly than I had moved on previous projects.

34
00:04:25,000 --> 00:04:30,000
So with any project model code base, you have a couple of kinds of unknowns.

35
00:04:30,000 --> 00:04:38,000
Known unknowns are the things that you know you don't know. If you started a list of things you don't know, you could slot those things into the list.

36
00:04:38,000 --> 00:04:44,000
These are more manageable than the other kind because being aware of these gaps means that you can plan for them.

37
00:04:44,000 --> 00:04:55,000
In machine learning, this could be knowing that you don't know which type of algorithm you're going to apply or knowing that you'll have to apply data transformations, but not being sure which ones.

38
00:04:55,000 --> 00:05:06,000
Unknown unknowns are the things that you don't know you don't know. So it would be impossible to list those things because your unknown unknowns are by definition unknown to you.

39
00:05:06,000 --> 00:05:13,000
You learn about these things as you're working through problems. And so a lot of them just kind of come up in hindsight. These are things that you didn't know.

40
00:05:13,000 --> 00:05:23,000
In a machine learning context, these could be things like dependencies that you definitely need to consider as you're implementing something, but you weren't aware of ahead of time.

41
00:05:23,000 --> 00:05:37,000
Today we'll focus on two frameworks that help speed up the process of getting ML to production or analyses or dashboards or new features. You can kind of insert your own thing here by efficiently conquering unknowns.

42
00:05:37,000 --> 00:05:45,000
So we'll start by tackling the known unknowns. How do we capture all of those things that we don't know and use that data to be more efficient?

43
00:05:45,000 --> 00:05:56,000
So we can borrow an idea from professional poker player and master decision maker, Annie Duke. Annie suggests that we start with a pre-mortem. So that's what we'll do.

44
00:05:56,000 --> 00:06:11,000
In order to tackle unknown unknowns, we want to make sure that we're not going to like trip over them. And so we'll dive into the second framework here, which is tracer bullets. We'll talk more about that in a minute.

45
00:06:11,000 --> 00:06:19,000
So we'll start with pre-mortems. What is a pre-mortem? A pre-mortem is an opportunity to predict possible outcomes for a project.

46
00:06:19,000 --> 00:06:26,000
Practically speaking, I think like the benefit you get here is a list of known reasons that a project could fail.

47
00:06:26,000 --> 00:06:35,000
This is a really good time to put all of the known unknowns into a list and figure out if anyone might be able to help you with answering some of those questions.

48
00:06:35,000 --> 00:06:45,000
If there are doom factors that would render a project like a total failure, missing a key deadline, for example, this is a really good place to discuss those things.

49
00:06:45,000 --> 00:06:55,000
I also want to note that I'm kind of framing this as a team exercise, but I found it really helpful to do pre-mortems, even as a solo exercise as I'm starting projects.

50
00:06:55,000 --> 00:07:14,000
I've also found that interviewing subject matter experts and using that as documentation is juice that's very worth the squeeze. So I'll often include pre-mortem-y questions about like where they think there might be areas that we need to be really careful about as I'm working with subject matter experts.

51
00:07:14,000 --> 00:07:26,000
And taking these things together and like building this list is a really good way to take all of the strands of the things that you know and like braid them into a constellation that will guide the work.

52
00:07:26,000 --> 00:07:33,000
So the idea of a pre-mortem is pretty simple. I wanted to show an example. This is Annie Duke's recommended format.

53
00:07:33,000 --> 00:07:44,000
She also adds sections around luck and skill. Coming from poker, she has this really interesting take on decision making and especially in her world you get feedback really quickly.

54
00:07:44,000 --> 00:07:51,000
So she's gotten really good at increasing the quality of her decisions while making really high quantities of decisions.

55
00:07:51,000 --> 00:07:56,000
And so I do like that framing. I didn't include it here, but you could.

56
00:07:56,000 --> 00:08:03,000
If you're thinking that this looks like a prioritization exercise, especially because of that column on the right, I would agree.

57
00:08:03,000 --> 00:08:11,000
Besides like getting the concerns into one place, it also gives you an idea of which places you might want to focus on first.

58
00:08:11,000 --> 00:08:21,000
I love a pre-mortem because I'm very good at guessing how things might fail and this is a rare opportunity to use that talent in a safe place where it is advancing the discussion.

59
00:08:21,000 --> 00:08:26,000
So they give people the opportunity to voice concerns without detracting.

60
00:08:26,000 --> 00:08:36,000
And I think that's super important because concerns are typically evidence of past experiences and learning from experience is one of the best ways for us to be better and more efficient in our work.

61
00:08:36,000 --> 00:08:48,000
So I love writing a pre-mortem. Now that we've accounted for known unknowns through the pre-mortem, we're aware of some things that could go wrong and we know where we might get tripped up.

62
00:08:48,000 --> 00:08:59,000
So armed with that knowledge, we can start building. To do that, we can move to our second framework, Tracer Bullets, for efficiently dealing with unknown knowns.

63
00:08:59,000 --> 00:09:05,000
So last year, our engineering department read The Pragmatic Programmer as a book club.

64
00:09:05,000 --> 00:09:13,000
This is a collection chock full of practical advice built on years of experience and it's solidified itself as an industry classic.

65
00:09:13,000 --> 00:09:22,000
I feel like it's very norm-comp. The stuff in there is not necessarily groundbreaking, but it's a really good read. It's something I wish I had read a lot sooner.

66
00:09:22,000 --> 00:09:30,000
Anyway, I loved it. I highly recommend it. And I've since adapted some of the ideas there for my own machine learning projects.

67
00:09:30,000 --> 00:09:38,000
And so specifically today, I want to talk about the idea of Tracer Bullets, which come from this book.

68
00:09:38,000 --> 00:09:47,000
So outside of a code base, in real life, Tracer Bullets are bullets that are loaded at intervals alongside regular ammunition.

69
00:09:47,000 --> 00:09:53,000
When they're fired, phosphorus ignites and leaves a pyrotechnic trail from the gun to whatever they hit.

70
00:09:53,000 --> 00:09:59,000
So you can see the path that each bullet takes and how close it gets to a given target.

71
00:09:59,000 --> 00:10:07,000
These are particularly useful for quickly refining your aim because they provide real-time feedback under actual conditions.

72
00:10:07,000 --> 00:10:11,000
So you're shooting these alongside anything else that you would be shooting.

73
00:10:11,000 --> 00:10:21,000
When you're building something that hasn't been built before, Tracer Bullets can visually illustrate the need for immediate feedback under actual conditions with a moving goal.

74
00:10:21,000 --> 00:10:29,000
So what does that look like in a code base? This is the visual that's in the Pragmatic Programmer.

75
00:10:29,000 --> 00:10:38,000
So each bar here represents a layer of logic, and you can see the Tracer Bullet goes through each layer of logic.

76
00:10:38,000 --> 00:10:45,000
When it intersects each layer, it carves out a small end-to-end path through all of the layers.

77
00:10:45,000 --> 00:10:54,000
The goal is to get through each layer with the absolute minimum amount of logic required before moving to the next one.

78
00:10:54,000 --> 00:11:03,000
So if we think like Peter in his earlier talk, we can say that we're building a bridge that's cheap enough that it barely stands, but it does stand.

79
00:11:03,000 --> 00:11:10,000
Tracer Bullet code operates in the same environment and under the same constraints that actual code would.

80
00:11:10,000 --> 00:11:16,000
And the focus on using Tracer Bullets in development is to get to the target quickly for more immediate feedback.

81
00:11:16,000 --> 00:11:24,000
Because Tracer Bullets don't require building a full project, they're a relatively cheap way to test an implementation.

82
00:11:24,000 --> 00:11:28,000
And so that's why we wanted to test them with the project I was working on.

83
00:11:28,000 --> 00:11:33,000
So I've translated this a bit to Tracer Bullets in machine learning.

84
00:11:33,000 --> 00:11:42,000
So we might start with a data pipeline, actually getting the data that we need to use, and then work through feature extraction.

85
00:11:42,000 --> 00:11:47,000
How do we make sure that we can get the data to a place where we can put it into a model or algo?

86
00:11:47,000 --> 00:11:53,000
And then we actually need to build the model or algo and deploy it, start to apply.

87
00:11:53,000 --> 00:12:05,000
In this case, it was doing classifications, so starting to actually do the classification on data and put it into an API or somewhere you can access that and then put like a front end on it.

88
00:12:05,000 --> 00:12:15,000
So how are we going to make sure that we're getting this stuff into the hands of users or whatever system is going to use the outputs?

89
00:12:15,000 --> 00:12:24,000
A couple of additional notes on Tracer Bullets and like how they're different from prototypes, because this is something I didn't understand until I read it a few times in the book.

90
00:12:24,000 --> 00:12:29,000
With a prototype, you're exploring specific aspects of a final system.

91
00:12:29,000 --> 00:12:37,000
And so you typically will throw that code away and recode properly once the lessons are learned.

92
00:12:37,000 --> 00:12:47,000
So if you've seen something like a design sprint, you kind of want to have something that you can show to stakeholders by the end of the week, and you might not be actually coding a whole thing.

93
00:12:47,000 --> 00:12:52,000
It's really just important that they can see like the UI or the front end or that last step.

94
00:12:52,000 --> 00:13:05,000
This is about answering questions along the way with the implementation. So you're doing like the minimum implementation possible so that you can see how your entire system works end to end.

95
00:13:05,000 --> 00:13:12,000
So Tracer code is lean, but it's complete. And it forms the parts of the skeleton of the final system.

96
00:13:12,000 --> 00:13:22,000
Once you have the skeleton built, I could choose to go work on like the tail for a while or the head for a while or flesh out the middle.

97
00:13:22,000 --> 00:13:30,000
But the entire system will continue to work with the Tracer code framework.

98
00:13:30,000 --> 00:13:39,000
So, this is where I needed to aim my Tracer Bullet. This is a quick architecture diagram from an early stage of the process and kind of what we were doing.

99
00:13:39,000 --> 00:13:49,000
So we have ETL logic, we have SRT files, these are like text files that are taking everything that's being said in a video and transcribing it.

100
00:13:49,000 --> 00:13:57,000
So that's all the text data that we're working with. We need to build logic to process that data and then to train the classifier.

101
00:13:57,000 --> 00:14:04,000
We need to actually get a classifier that works and is good enough for our use case. That was one of those open questions that we had.

102
00:14:04,000 --> 00:14:12,000
And then we need to be able to output the closest lesson. And so this is the example of the stuff that we wanted to deliver.

103
00:14:12,000 --> 00:14:18,000
A lot of this lived in a Jupyter notebook. We actually decided to put like a front end on it so that people could interact with it.

104
00:14:18,000 --> 00:14:32,000
But I think having this clear path for like, I need my Tracer Bullet to go through every single one of these layers was really helpful to answer questions that we had around architecture and like, would this thing even work?

105
00:14:32,000 --> 00:14:46,000
So firing the Tracer Bullet, we built the data pipeline, did feature extraction, built the classifier, started doing classifications, made sure all of that stuff was up to spec, and then added the front end.

106
00:14:46,000 --> 00:14:57,000
But something important here is like what's left after the Tracer Bullet. So if you look at like the pink area, that would be the part that was included or the part that was coded out as part of the Tracer Bullet.

107
00:14:57,000 --> 00:15:09,000
There is lots left to do at each stage here. So for example, the data pipeline, I didn't grab all of the videos or the files containing like the text transcriptions of the videos.

108
00:15:09,000 --> 00:15:18,000
Some of those were still left on the table. They weren't necessary for the implementation. Feature extraction, some of the work that we were doing around like tokenizing, stemming, etc.

109
00:15:18,000 --> 00:15:29,000
We could change in future iterations, but we had like enough to build a model that was a good proof of concept. The machine learning code, we had a good idea for a model that worked.

110
00:15:29,000 --> 00:15:34,000
Obviously, that's something we could go back and update, but having it as part of the pipeline was really important.

111
00:15:34,000 --> 00:15:44,000
And then the API and the user interface, we were able to get feedback on. And so if there's anything that we needed to change, we'd be able to do that pretty easily.

112
00:15:44,000 --> 00:15:51,000
And so this was really helpful to have an idea of just like we're leaving some things on the table to come back and do.

113
00:15:51,000 --> 00:15:58,000
But also, this doesn't mean that we're waiting, for example, for the entire data pipeline to be built before we're moving on to the next thing.

114
00:15:58,000 --> 00:16:13,000
So the advantage is that you can get to delivering something or shipping something faster, which is really helpful for stakeholder management too, especially if you're working with something where there's a lot of open questions.

115
00:16:13,000 --> 00:16:22,000
I built this slide about tracer bullet driven development. And so this would be like, let's say you had five checkpoints and you were working as part of a team.

116
00:16:22,000 --> 00:16:33,000
If you look at this first approach, this is if we're building sequentially. So we start with like the data sets and then the feature engineering and then the modeling and then serving the model and then having a front end.

117
00:16:33,000 --> 00:16:41,000
That takes a long time before we're actually seeing results and having an idea for whether the thing works or not.

118
00:16:41,000 --> 00:16:46,000
But if we take that same amount of work, if we assume it takes the same amount of work at each checkpoint,

119
00:16:46,000 --> 00:16:52,000
we can see that we're delivering like a V1, the first checkpoint, and then V2 and then V3 and V4 and V5.

120
00:16:52,000 --> 00:16:59,000
And so from a stakeholder perspective, this was really helpful to be able to say like, oh, look, we have this thing working end to end.

121
00:16:59,000 --> 00:17:06,000
We're going to improve the accuracy. That's no problem. It's just updating information and like steps one, two and three.

122
00:17:06,000 --> 00:17:15,000
So I really liked using the tracer bullets, especially for working with stakeholders and reducing uncertainty for them.

123
00:17:15,000 --> 00:17:24,000
And the other nice thing about using these two frameworks together is so the known unknowns, once you have those and you know the areas that could trip you up,

124
00:17:24,000 --> 00:17:30,000
you can build your system around them and make sure that they're included as part of the layers for the tracer bullets.

125
00:17:30,000 --> 00:17:36,000
So you kind of know based on those, like what are the layers you need to fire the tracer bullet through?

126
00:17:36,000 --> 00:17:42,000
And then you're able to fire the tracer bullet, make sure that you're accounting for at least everything you know about.

127
00:17:42,000 --> 00:17:46,000
That gives you the opportunity to find things that you didn't know about because you're actually building it.

128
00:17:46,000 --> 00:17:49,000
That always happens. You're always going to have surprises.

129
00:17:49,000 --> 00:17:54,000
But it gets you from the start of the project to the end of the project a lot faster.

130
00:17:54,000 --> 00:18:01,000
And with some check in points where you can offload information or share information. And I found that really helpful and really exciting.

131
00:18:01,000 --> 00:18:08,000
It's something I want to try more and more with future projects.

132
00:18:08,000 --> 00:18:13,000
And so I wanted to say thank you so much to the NormConf organizers for getting everyone together.

133
00:18:13,000 --> 00:18:19,000
This has been great. I've been watching all morning and loving it. For anyone who wants to learn more about these topics,

134
00:18:19,000 --> 00:18:26,000
there's a couple of books that I recommend. So I talked earlier about the Pragmatic Programmer. That's a classic. It's really good.

135
00:18:26,000 --> 00:18:32,000
Annie Dupes' Thinking in Bets is also fantastic. She has a couple of other frameworks that I've used.

136
00:18:32,000 --> 00:18:40,000
Things like backcasting. So if you have an idea of what success looks like, similar to like Amazon's PR approach,

137
00:18:40,000 --> 00:18:44,000
where they start with like, what does the press release look like? And then you work backwards.

138
00:18:44,000 --> 00:18:53,000
She basically applies the premortem but to success. So if we're successful, what are the points that we have to hit along the way?

139
00:18:53,000 --> 00:19:00,000
So those books are great. And if you want to learn more from me, I'm at Bianna Poesy on Twitter and Rexis.social.

140
00:19:00,000 --> 00:19:15,000
And I also have a blog at CaitlinHuden.com.

